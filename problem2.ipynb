{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "In this question you will analyze different ML cloud platforms and compare their service offerings. In particular, you will consider ML cloud offerings from IBM, Google, Microsoft, and Amazon and compare them on the basis of following criteria:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Frameworks: DL framework(s) supported and their version. \n",
    "\n",
    "Here we are referring to machine learning platforms which have their own inbuilt images for different frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                             |    Amazon    |         IBM        |    Google    |      Microsoft      |\n",
    "|:---------------------------:|:------------:|:------------------:|:------------:|:-------------------:|\n",
    "|           Pytorch           | $\\checkmark$ | $\\checkmark$ v1.10 | $\\checkmark$ | $\\checkmark$ v1.9.0 |\n",
    "|          Tensorflow         | $\\checkmark$ |  $\\checkmark$ v2.7 | $\\checkmark$ |  $\\checkmark$ v2.5  |\n",
    "|            Caffe            |              |                    | $\\checkmark$ |                     |\n",
    "|         Apache MXNet        | $\\checkmark$ |                    | $\\checkmark$ |                     |\n",
    "|            Keras            | $\\checkmark$ |                    |              |                     |\n",
    "|             SPSS            |              | $\\checkmark$ v18.2 |              |                     |\n",
    "|            Spark            |              |  $\\checkmark$ v3.0 |              |                     |\n",
    "|           Chainer           | $\\checkmark$ |                    | $\\checkmark$ |                     |\n",
    "| Microsoft Cognitive Toolkit | $\\checkmark$ |                    |              |                     |\n",
    "|        Hybrid/AutoAI        |              |  $\\checkmark$ v0.1 |              |                     |\n",
    "|            Gluon            | $\\checkmark$ |                    |              |                     |\n",
    "|           Horovod           | $\\checkmark$ |                    |              |  $\\checkmark$ v0.21 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the version is not shown, it is because this information is not in the documentation of the cloud provider, and I'm not going to go through each on of them doing --version on each provider for each framework. Also, sometimes, the framework is supported, but it is not clear if you can set up an image with the framework directly or not suck as Keras in google cloud.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Compute units: type(s) of compute units offered, i.e., GPU types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                          |    Amazon    |     IBM      |    Google    |   Microsoft  |\n",
    "|:------------------------:|:------------:|:------------:|:------------:|:------------:|\n",
    "|        NVIDIA A100       | $\\checkmark$ |              | $\\checkmark$ |              |\n",
    "|        NVIDIA M60        | $\\checkmark$ | $\\checkmark$ |              | $\\checkmark$ |\n",
    "|         NVIDIA T4        | $\\checkmark$ |              | $\\checkmark$ | $\\checkmark$ |\n",
    "|        NVIDIA T4G        | $\\checkmark$ |              |              |              |\n",
    "|        NVIDIA A10G       | $\\checkmark$ |              |              |              |\n",
    "|        NVIDIA A10        |              |              |              | $\\checkmark$ |\n",
    "|        NVIDIA A100       |              |              |              | $\\checkmark$ |\n",
    "|        NVIDIA P100       |              | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |\n",
    "|        NVIDIA V100       | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |\n",
    "|         NVIDIA P4        |              |              | $\\checkmark$ |              |\n",
    "|        NVIDIA P40        |              |              |              | $\\checkmark$ |\n",
    "|        NVIDIA K80        | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |\n",
    "| AMD Radeon Instinct MI25 |              |              |              | $\\checkmark$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For AWS, it was hard to find a list of available GPUs. And microsoft is just weird with the names they use for the machines using GPU's, but in the documentation for each one of them, they mention which nvidia GPU they are using under the hood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model lifecycle management: tools supported to manage ML model lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS\n",
    "\n",
    "It seems like there is a tool in AWS called [SageMaker MLPOps](https://aws.amazon.com/sagemaker/mlops/) which allows the user to build CI/CD pipelines to reduce model management overhead, and it allows the user to automates ML workflows. Apparently it also monitors the quality of models, detects bias, model drift, and also detects concept drift. Finally, it says that it can \"Automatically track code, datasets, and artifacts at every step of the ML lifecycle for governance\". which is the specific thing that we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM\n",
    "\n",
    "In the IBM cloud the have [Pak](https://www.ibm.com/products/cloud-pak-for-data) They also have a [tutorial](https://www.ibm.com/cloud/blog/ai-model-lifecycle-management-overview) explaining how it can be used to manga the lifecycle of a model. They explain that it does pretty much what we expect a management tool like this would do. It helps with training and deployment, it has a visualization of the whole pipeline, it has connectors to data sources, it provides the ability to deploy and train at scale, helps monitor data quality, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud\n",
    "\n",
    "They have several tools, one of them is [Vertex](https://cloud.google.com/vertex-ai), which allows the user to build and deploy models with minimal code and provides MLOps tools. They also have [AI Platform](https://cloud.google.com/ai-platform/docs/technical-overview) which can be used to manage every stage of the ML pipeline (except for coding the model). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft Azure\n",
    "\n",
    "It seems like they don't really have a specific tool for this. They do provide a [guide for MLOps](https://docs.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment), but nothing else. In the guide they suggest several tools for building your own pipeline so you are on your own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Monitoring: availability of application logs and resource (GPU, CPU, memory) usage monitoring data to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS\n",
    "\n",
    "AWS does provide a tool to do exactly this, it is called [CloudWatch](https://aws.amazon.com/blogs/machine-learning/monitoring-gpu-utilization-with-amazon-cloudwatch/). It provides the user with the data for GPU and CPU utilization. It also provides the user the application logs from the instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM\n",
    "\n",
    "IBM also has a tool for providing logs to the user, it called [IBM Log Analysis](https://www.ibm.com/cloud/log-analysis). For GPU and CPU monitoring, they seem to have a service called [IBM Cloud Monitoring](https://www.ibm.com/cloud/cloud-monitoring) but it seems to be targeted towards DevOps rather than the ML community. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud\n",
    "\n",
    "In GCP, it is possible to see metrics such as CPU, GPU, and memory utilization in the Metrics Explorer page as described [here](https://cloud.google.com/compute/docs/gpus/monitor-gpus). In terms of logs, they have something called [Cloud Logging](https://cloud.google.com/logging) which apparently is a log management tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft Azure\n",
    "\n",
    "In azure you can also view the logs by enabling application logging. I could not find any tool for GPU, CPU, etc, monitoring, but they hae the ability to create dashboards, so they recommend [creating one with Azure Monitor](https://techcommunity.microsoft.com/t5/azure-global/gpu-monitoring-using-azure-monitor/ba-p/3107511) to track metrics like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Visualization during training: performance metrics like accuracy and throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS\n",
    "\n",
    "Yes, Amazon SageMaker algorithms provide built-in support for metrics and they can shown in a dashboard. They also provide the option to define your own metrics for custom training algorithms. (I believe this is also part of CloudWatch) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM\n",
    "\n",
    "They do have the ability to create a dashboard based on metrics using their [Pak](https://dataplatform.cloud.ibm.com/docs/content/wsj/model/cloud-performance_metrics.html) tool, but it is unclear if they have a tool other than watson studio to do visualization during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud\n",
    "\n",
    "They suggest to [use TensorBoard](https://cloud.google.com/ai-platform/training/docs/monitor-training) for monitoring these kind of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft Azure\n",
    "They don't provide any tools specific for this, but they do provide a way of [saving training metrics](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-log-view-metrics) and logs and also a way of seeing these metrics in the studio UI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Elastic Scaling: support for elastic scaling compute resources of an ongoing job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS\n",
    "\n",
    "AWS has something called Auto Scaling. According to their docs, \"AWS Auto Scaling lets you build scaling plans that automate how groups of different resources respond to changes in demand\", which means they don't have this problem if you configure your jobs correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM\n",
    "\n",
    "They also have autoscaling. They say, \"Autoscaling is designed to respond to the short-to-medium term trends in resource usage on your IBM CloudÂ® Databases for Elasticsearch deployment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud\n",
    "They also have autoscaling, similar to the previous ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft Azure\n",
    "\n",
    "Also autoscaling. In their website, they say, \"Scale apps to meet changing demand with Azure Autoscale. Autoscale is a built-in feature of Cloud Services, Mobile Services, Virtual Machines, and Websites.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Training job description: training job description file format. Show how the same training job is specified in different ML platforms as YAML file. Identify similar fields in the training job file for any two ML platforms through an example.\n",
    "\n",
    "This question is slightly ambiguous, so I don't know if it is asking for a comparison of all four cloud providers in terms of training job description, or just a comparison between two platforms that use YAML files, but I'm going to give both answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS\n",
    "\n",
    "It has its own custom format.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM\n",
    "\n",
    "Apparently in Cloud Pak for Data uses a YAML file that contains different fields describing the model to be trained, including the deep learning framework to use, the location of the data, and several arguments required for model execution during training and testing. \n",
    "\n",
    "https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml_dlaas_working_with_training_definitions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud\n",
    "\n",
    "They use flags in the command for running the job, as well as a configuration file in the form of a YAML file. \n",
    "\n",
    "https://cloud.google.com/ai-platform/training/docs/training-jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft Azure\n",
    "\n",
    "In Azure, there is a way of specifying an ML pipeline in YAML. \n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/reference-pipeline-yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now we compare the YAML files from IBM with GCP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: complete this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
