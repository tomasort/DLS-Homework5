{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Explain the difference between episodic and continuous tasks? Given an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between episodic and continuous tasks is that in the case of continuous tasks, the task might go on forever without terminating, while episodic tasks will have a terminal state that will be reached in a finite amount of time. An episodic task could be a car racing game, where one race could be considered a single episode. In the case of continuous, one example could be a robot who's task is to be a personal assistant or a trading bot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) What do the terms exploration and exploitation mean in RL ? Why do the actors employ ϵ-greedy policy for selecting actions at each step? Should ϵ remain fixed or follow a schedule during Deep RL training ? How does the value of ϵ help balance exploration and exploitation during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration refers to the system's attempts to discover new features about the world by selecting actions that are not optimal (or are not part of the current policy). On the other hand, exploitation simply refers to using the things that the system already knows to get the best possible result or in other words, to maximize the reward.\n",
    "\n",
    "Actors employ $\\epsilon$-greedy policy because they want to allow some exploration while mainly maintaining the greedy policy. According to the paper, the value of $\\epsilon$ starts at 1, and is annealed linearly to a small value of 0.1 over over the ﬁrst million frames, and ﬁxed at 0.1 thereafter.\n",
    "\n",
    "If $\\epsilon = 0$, then the policy becomes the greedy policy, and on the other hand, if $\\epsilon = 1$, then the actor will always explore. This means that by changing the value of $\\epsilon$, we will change how much exploration and exploitation we do. As we increase $\\epsilon$, the amount of exploitation decreases, while if we decrease $\\epsilon$, then the the amount of exploration also decreases. It seems to be a good idea to keep this value low to allow for some exploration while maximizing the reward.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) How is the Deep Q-Learning algorithm different from Q-learning ? You will follow the steps of Deep Q-Learning algorithm in Mnih et al. (2013) page 5, and explain each step in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between Deep Q-Learning and regular Q-learning is the use of neural networks to map input states to actions and q-values. I also think that the use of experience replay is also an important feature of deep Q-learning. \n",
    "\n",
    "Below is line by line explanation of the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deepQlearning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first line, we initialize the experience replay buffer $D$. When then treat $Q$ as a neural network and initialize it with random weights. \n",
    "\n",
    "In the third line we start going episode by episode, and the first thing we do is preprocess the sequence of frames and build the correct input. \n",
    "\n",
    "Then in the second for loop, we go through each step and determine which action we are going to take next (with $\\epsilon$-greedy policy as explained in the previous questions). We then observe the reward and the new state.\n",
    "\n",
    "We then set the next state $s_{t+1} = s_t, a_t, x_{t+1}$ and then we store this transition in the replay buffer. \n",
    "\n",
    "Finally, we sample randomly from $D$ to get the input for the deep neural network, and to get the value of y to be able to train it, we first determine if this is the end of the episode or not. If it is the end, then y is simply the reward, else, y is the reward plus the reward that we will get in the future. To complete this iteration, we train the network using gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) What is the benefit of having a target Q-network ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target Q-network is important because it stabilizes learning. According to the paper, \"The target Q-network is used to generate target Q values for the DQN loss term and the replay memory that the agent uses to sample random transitions for training the Q-network\". As I understand it, the Q-network is updated every step, this could introduce a lot of noise so we copy the weights from the Q-network into the target Q-network to avoid this. This also keeps the target function from changing too quickly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) How does experience replay help in efficient Q-learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) What is prioritized experience replay and how is priority of a sample calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Compare and contrast GORILA (General Reinforcement Learning Architecture) and Ape-X architecture. Provide three similarities and three differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Why the performance improves with number of actors in Ape-X?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
