{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Explain the difference between episodic and continuous tasks? Given an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The difference between episodic and continuous tasks is that in the case of continuous tasks, the task might go on forever without terminating, while episodic tasks will have a terminal state that will be reached in a finite amount of time. An episodic task could be a car racing game, where one race could be considered a single episode. In the case of continuous, one example could be a robot who's task is to be a personal assistant or a trading bot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What do the terms exploration and exploitation mean in RL ? Why do the actors employ ϵ-greedy policy for selecting actions at each step? Should ϵ remain fixed or follow a schedule during Deep RL training ? How does the value of ϵ help balance exploration and exploitation during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exploration refers to the system's attempts to discover new features about the world by selecting actions that are not optimal. On the other hand, exploitation simply refers to using the things that the system already know to get the best possible result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) How is the Deep Q-Learning algorithm different from Q-learning ? You will follow the steps of Deep Q-Learning algorithm in Mnih et al. (2013) page 5, and explain each step in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) What is the benefit of having a target Q-network ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) How does experience replay help in efficient Q-learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) What is prioritized experience replay and how is priority of a sample calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Compare and contrast GORILA (General Reinforcement Learning Architecture) and Ape-X architecture. Provide three similarities and three differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Why the performance improves with number of actors in Ape-X?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
