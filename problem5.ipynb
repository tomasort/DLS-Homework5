{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download CIFAR10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvcc: command not found\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is TTA stability? \n",
    "What does it mean for a model to be optimized for TTA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we compute the coefficient of variation of TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned on campuswire, we use RTX8000 instead of TPU pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Resnet50 implementation that we will use in this assignment is from this [github repo](https://github.com/akamaster/pytorch_resnet_cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch_resnet_cifar10'...\n",
      "remote: Enumerating objects: 91, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 91 (delta 1), reused 5 (delta 1), pack-reused 84\u001b[K\n",
      "Receiving objects: 100% (91/91), 84.32 MiB | 36.08 MiB/s, done.\n",
      "Resolving deltas: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone git@github.com:akamaster/pytorch_resnet_cifar10.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of params 755802\n",
      "Total layers 50\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'pytorch_resnet_cifar10')\n",
    "from resnet import ResNet, BasicBlock, test\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(BasicBlock, [8, 8, 8])\n",
    "\n",
    "net = resnet50()\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import I\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "from os import path\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def setup_logging(log_file='log.txt', resume=False):\n",
    "    \"\"\"\n",
    "    Setup logging configuration\n",
    "    \"\"\"\n",
    "    file_mode = 'a' if os.path.isfile(log_file) and resume else 'w'\n",
    "\n",
    "    root_logger = logging.getLogger()\n",
    "    logging.basicConfig(level=logging.DEBUG,\n",
    "                        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "                        datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Remove all existing handlers (can't use the `force` option with\n",
    "    # python < 3.8)\n",
    "    for hdlr in root_logger.handlers[:]:\n",
    "        root_logger.removeHandler(hdlr)\n",
    "    # Add the handlers we want to use\n",
    "    fileout = logging.FileHandler(log_file, mode=file_mode)\n",
    "    fileout.setLevel(logging.DEBUG)\n",
    "    fileout.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "    logging.getLogger().addHandler(fileout)\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    console.setFormatter(logging.Formatter('%(message)s'))\n",
    "    logging.getLogger().addHandler(console)\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=200, scheduler=None, save_path=\"./\", show_iteration_info=False, model_name=None, colab=False, limit_num_epochs=20):\n",
    "    \"\"\"\n",
    "    Function to train a deep learning model. \n",
    "        model: a pytorch model \n",
    "        dataloaders: a dictionary with 'train' and 'val' keys with their respective dataloaders\n",
    "        criterion: the loss function \n",
    "        optimizer: the optimizer to be used\n",
    "        num_epochs: number of epochs to train\n",
    "        scheduler: if it is specified, the scheduler will be used\n",
    "        save_path: the directory where we can save the model and the log output.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    path_created = False\n",
    "    # Check whether the specified path exists or not\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create a new directory because it does not exist \n",
    "        os.makedirs(save_path)\n",
    "        path_created = True\n",
    "\n",
    "    setup_logging(os.path.join(save_path, 'log.txt'))\n",
    "\n",
    "    if path_created:\n",
    "        logging.info(f\"'{save_path}' Directory was created!\")\n",
    "\n",
    "    logging.info(f\"Starting training on: device={device}\\n\")\n",
    "    # Print information about the hardware used in google colab\n",
    "    # if colab:\n",
    "    #     gpu_info = !nvidia-smi\n",
    "    #     gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
    "    #     gpu_info = '\\n'.join(gpu_info)\n",
    "    #     if gpu_info.find('failed') >= 0:\n",
    "    #         logging.info('Not connected to a GPU')\n",
    "    #     else:\n",
    "    #         logging.info(gpu_info)\n",
    "    #     logging.info(gpu_name)\n",
    "\n",
    "    if model_name is not None:\n",
    "        logging.info(f\"Training {model_name}\")\n",
    "\n",
    "    logging.info(\"Model information: \")\n",
    "    total_params = 0\n",
    "    for x in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        total_params += np.prod(x.data.numpy().shape)\n",
    "    logging.info(f\"\\tTotal number of params: {round(total_params/1e6, 2)}M\")\n",
    "    logging.info(f\"\\tTotal layers {len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, model.parameters())))}\")\n",
    "\n",
    "    model_name = '' if model_name is None else model_name\n",
    "    logging.info(f\"Optimizer: {optimizer}\")\n",
    "    if scheduler is not None:\n",
    "        logging.info(f\"Scheduler: {scheduler.state_dict()}\")\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    df = pd.DataFrame(columns=[\"epoch\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"])\n",
    "    model.to(device)\n",
    "    best_acc_updated_on = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        logging.info('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        logging.info('-' * 10)\n",
    "        info = {'train': None, 'val': None}\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_number_of_samples = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_number_of_samples += labels.shape[0]\n",
    "                report = str('{phase} - Epoch: [{0}][{1}/{2}]\\t'\n",
    "                            f'Loss {loss.item()} (avg: {running_loss/running_number_of_samples})\\t'\n",
    "                            f'Acc {torch.sum(preds == labels.data)/labels.shape[0]}, (avg: {running_corrects/running_number_of_samples})\\t'\n",
    "                            .format(\n",
    "                                epoch, i, len(dataloaders[phase]),\n",
    "                                phase=phase))  \n",
    "                if show_iteration_info: \n",
    "                    logging.info(report)\n",
    "    \n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            logging.info('{} Loss: {:.4f} Acc: {:.4f} \\t model: {}'.format(phase, epoch_loss, epoch_acc, model_name))\n",
    "            info[phase] = (epoch_loss, epoch_acc.item())\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc_updated_on = epoch\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                logging.info(f\"best acc: {best_acc:.4f}\")\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "\n",
    "        # Save info on dataframe \n",
    "        train_loss = info['train'][0]\n",
    "        train_acc = info['train'][1]\n",
    "        test_loss = info['val'][0]\n",
    "        test_acc = info['val'][1]\n",
    "\n",
    "        df.loc[len(df.index)] = [epoch, train_loss, train_acc, test_loss, test_acc]\n",
    "\n",
    "        # if  epoch - best_acc_updated_on > limit_num_epochs:\n",
    "        #     logging.info(f\"Terminating training because accuracy has not improved in {limit_num_epochs} epochs\")\n",
    "        #     break\n",
    "\n",
    "        # make a step on the scheduler every epoch\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            logging.info(f\"Scheduler: {scheduler.state_dict()}\")\n",
    "\n",
    "        logging.info(\"\")\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    logging.info('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    logging.info('Best val Acc: {:4f}'.format(best_acc))\n",
    "    df.to_csv(os.path.join(save_path, 'training_info.csv'))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "\n",
    "def get_conv_info(layer, input_shape, spaces=''):\n",
    "    \"\"\"Compute information from convolution layer\"\"\"\n",
    "    # Get input information from the layer\n",
    "    batch_size = input_shape[0]\n",
    "    I_c = input_shape[1]\n",
    "    I_w = input_shape[2]\n",
    "    I_h = input_shape[3]\n",
    "    k = layer.weight.shape[-1]\n",
    "    logging.info(f'{spaces}kernel:{k=}x{k=}')\n",
    "    s = layer.stride[0]\n",
    "    logging.info(f'{spaces}stride:{s=}')\n",
    "    p = layer.padding[0]\n",
    "    logging.info(f'{spaces}padding:{p=}')\n",
    "    # Calculate output information\n",
    "    O_c = layer.weight.shape[0] \n",
    "    O_w = floor((I_w - k + 2*p)/s) + 1\n",
    "    O_h = floor((I_h - k + 2*p)/s) + 1\n",
    "    logging.info(f'{spaces}Input Size: {I_w=} {I_h=} {I_c=}')\n",
    "    logging.info(f\"{spaces}Output size: {O_w=} {O_h=} {O_c=}\")\n",
    "    logging.info(f\"{spaces}Weights per filter: {k=}x{k=}x{I_c=} = {k*k*I_c}\")\n",
    "    logging.info(f\"{spaces}Parameters: {k=}x{k=}x{I_c=}x{O_c=} + {O_c=} biases = {k*k*I_c*O_c + O_c:,}\")\n",
    "    logging.info(f\"{spaces}Number of Multiply-Adds: ({I_h=}x{I_w=}x{I_c=})x({k=}x{k=}x{O_c=}) = {I_h*I_w*I_c*k*k*O_c:,}\")\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # This part is important for calculating flops from Lu. et al \"Modeling the resource requirements ...\"\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    matrix = (O_h*O_w)*(k*k*I_c)*(k*k*I_c)*(O_c)\n",
    "    logging.info(f\"{spaces}Matrix Multiplication size: [({O_h=}x{O_w=})x({k=}x{k=}x{I_c=})][({k=}x{k=}x{I_c=})x({O_c=})] = {(O_h*O_w)*(k*k*I_c):,}x{(k*k*I_c)*(O_c):,} = {matrix:,}\")\n",
    "    return k*k*I_c*O_c\n",
    "\n",
    "def get_layer_info(index, layer, input_shape, output, biases=False, spaces=''):\n",
    "    \"\"\" logging.infos the information of a certain layer\n",
    "        Returns the activation number and the number of parameters \"\"\"\n",
    "    activations = output.numel()\n",
    "    parameters = 0\n",
    "    if isinstance(layer, torch.nn.ReLU) :\n",
    "        return 0, 0 \n",
    "    logging.info(f\"{spaces}{index}) {layer.__class__.__name__}\")\n",
    "    if isinstance(layer, torch.nn.Flatten) or isinstance(layer, torch.nn.Dropout):\n",
    "        return 0, 0 \n",
    "    if isinstance(layer, torch.nn.Sequential):\n",
    "        for i, layer_layer in enumerate(layer):\n",
    "            get_layer_info(f\"{index}_{i}\", layer_layer, input_shape, output, biases, spaces=spaces*2 if len(spaces) > 0 else \"    \")\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        get_conv_info(layer=layer, input_shape=input_shape, spaces=spaces*2 if len(spaces) > 0 else \"    \")\n",
    "    logging.info(f'{spaces*2}* number of activations: output_shape={output.shape}, -> {output.numel():,}')\n",
    "    # We only count the linear layer and conv layers\n",
    "    if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.Conv2d):\n",
    "        params = list(layer.parameters())\n",
    "        logging.info(f\"{spaces*2}Weights:\", params[0].shape)\n",
    "        logging.info(f\"{spaces*2}Biases:\", params[1].shape)\n",
    "        if biases:\n",
    "            parameters = parameters + params[1].shape[0]\n",
    "        parameters = parameters + layer.weight.numel()\n",
    "        logging.info(f'{spaces*2}- parameters: weights={layer.weight.shape}, -> {parameters:,}')\n",
    "    return activations, parameters\n",
    "\n",
    "\n",
    "def get_parameters(model, input_size=(3, 200, 200), biases=False):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X = torch.randn(input_size).unsqueeze(0).to(device)\n",
    "    total_activations = []\n",
    "    total_parameters = []\n",
    "    logging.info(\"Input:\", \n",
    "            f'\\n    number of activations: {X.shape}, {X.numel():,}')\n",
    "    total_activations.append(X.numel())\n",
    "    for i, layer in enumerate(model):\n",
    "            input_shape = X.shape\n",
    "            X=layer(X)\n",
    "            activations, parameters = get_layer_info(i, layer, input_shape=input_shape, output=X, biases=biases)\n",
    "            total_parameters.append(parameters)\n",
    "            total_activations.append(activations)\n",
    "    logging.info(\"\\n\")\n",
    "    parameters = sum(total_parameters)\n",
    "    activations = sum(total_activations)\n",
    "    logging.info(f\"Total number of Parameters: {parameters:,}\")\n",
    "    logging.info(f\"Total number of Activations: {activations:,}\")\n",
    "    return parameters, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4d1d05e769fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_to_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmilestones\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "model = resnet50()\n",
    "params_to_update = model.parameters()\n",
    "lr = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(params_to_update, lr=lr, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 180], gamma=0.1)\n",
    "train_model(model, dataloaders, criterion, optimizer, num_epochs=350, scheduler=scheduler, save_path=args.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
