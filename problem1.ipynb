{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkE3viQ519h1"
   },
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFNZqtEL19h3",
    "outputId": "99a28406-57eb-4eac-e1b1-cbc086da0812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-ssd'...\n",
      "remote: Enumerating objects: 812, done.\u001b[K\n",
      "remote: Total 812 (delta 0), reused 0 (delta 0), pack-reused 812\u001b[K\n",
      "Receiving objects: 100% (812/812), 1.05 MiB | 25.60 MiB/s, done.\n",
      "Resolving deltas: 100% (544/544), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/qfgaohao/pytorch-ssd.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLfB54z5FLMY"
   },
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vnipncaq264H",
    "outputId": "a475f4c4-932f-4f49-807b-64cdfa471519"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# !tar -xf gdrive/MyDrive/DLS-Homework5/VOCtest_06-Nov-2007.tar -C ./\n",
    "!tar -xf ../data/VOCtest_06-Nov-2007.tar -C ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_NNOaw9FHSj"
   },
   "source": [
    "Download pretrained pytorch MobilenetV1 SSD and test it locally using Pascal VOC 2007 dataset.\n",
    "\n",
    "Show the test accuracy for the 20 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8gsep9R2SFO",
    "outputId": "e88f8024-9b57-4a49-8df8-e3c6f07a6589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-05 05:27:30--  https://storage.googleapis.com/models-hao/mobilenet-v1-ssd-mp-0_675.pth\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.80.48, 142.250.80.80, 142.251.40.112, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.80.48|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 37995286 (36M) [application/octet-stream]\n",
      "Saving to: ‘models/mobilenet-v1-ssd-mp-0_675.pth’\n",
      "\n",
      "mobilenet-v1-ssd-mp 100%[===================>]  36.23M  80.4MB/s    in 0.5s    \n",
      "\n",
      "2022-05-05 05:27:31 (80.4 MB/s) - ‘models/mobilenet-v1-ssd-mp-0_675.pth’ saved [37995286/37995286]\n",
      "\n",
      "--2022-05-05 05:27:31--  https://storage.googleapis.com/models-hao/voc-model-labels.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.176.208, 142.251.35.176, 142.251.40.240, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.176.208|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 145 [text/plain]\n",
      "Saving to: ‘models/voc-model-labels.txt’\n",
      "\n",
      "voc-model-labels.tx 100%[===================>]     145  --.-KB/s    in 0s      \n",
      "\n",
      "2022-05-05 05:27:31 (156 MB/s) - ‘models/voc-model-labels.txt’ saved [145/145]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P models https://storage.googleapis.com/models-hao/mobilenet-v1-ssd-mp-0_675.pth\n",
    "!wget -P models https://storage.googleapis.com/models-hao/voc-model-labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEG4WBlvFQ8T"
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in /home/tor213/.local/lib/python3.8/site-packages (0.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/tor213/.local/lib/python3.8/site-packages (from torchvision) (3.10.0.2)\n",
      "Requirement already satisfied: torch==1.11.0 in /home/tor213/.local/lib/python3.8/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: requests in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torchvision) (2.24.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torchvision) (8.0.1)\n",
      "Requirement already satisfied: numpy in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/numpy-1.19.2-py3.8-linux-x86_64.egg (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests->torchvision) (2.10)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nvidia-pyindex\n",
      "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
      "Building wheels for collected packages: nvidia-pyindex\n",
      "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8399 sha256=196affb539a4d0e23b97b326c3158e230da67a8a7873ce1a75e6f8b693746049\n",
      "  Stored in directory: /home/tor213/.cache/pip/wheels/e0/c2/fb/5cf4e1cfaf28007238362cb746fb38fc2dd76348331a748d54\n",
      "Successfully built nvidia-pyindex\n",
      "Installing collected packages: nvidia-pyindex\n",
      "Successfully installed nvidia-pyindex-1.0.9\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pytorch-quantization\n",
      "  Downloading https://developer.download.nvidia.com/compute/redist/pytorch-quantization/pytorch_quantization-2.1.2-cp38-cp38-linux_x86_64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 66.7 MB/s eta 0:00:01                   | 256 kB 66.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting prettytable\n",
      "  Downloading prettytable-3.2.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: scipy in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/scipy-1.5.2-py3.8-linux-x86_64.egg (from pytorch-quantization) (1.5.2)\n",
      "Collecting sphinx-glpi-theme\n",
      "  Downloading sphinx_glpi_theme-0.3-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 58.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from pytorch-quantization) (5.3.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from pytorch-quantization) (0.13.0)\n",
      "Requirement already satisfied: numpy in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages/numpy-1.19.2-py3.8-linux-x86_64.egg (from pytorch-quantization) (1.19.2)\n",
      "Requirement already satisfied: wcwidth in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from prettytable->pytorch-quantization) (0.2.5)\n",
      "Requirement already satisfied: six in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from absl-py>=0.7.0->pytorch-quantization) (1.15.0)\n",
      "Installing collected packages: prettytable, sphinx-glpi-theme, pytorch-quantization\n",
      "Successfully installed prettytable-3.2.0 pytorch-quantization-2.1.2 sphinx-glpi-theme-0.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "!pip install nvidia-pyindex\n",
    "!pip install pytorch-quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qR2PpxRY19h5",
    "outputId": "8f685bc0-d980-4841-a876-92103312f608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tor213/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/tor213/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZNK2at10TensorBase21__dispatch_contiguousEN3c1012MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"pytorch-ssd/eval_ssd.py\", line 2, in <module>\n",
      "    from vision.ssd.vgg_ssd import create_vgg_ssd, create_vgg_ssd_predictor\n",
      "  File \"/scratch/tor213/DLS-Homework5/pytorch-ssd/vision/ssd/vgg_ssd.py\", line 6, in <module>\n",
      "    from .predictor import Predictor\n",
      "  File \"/scratch/tor213/DLS-Homework5/pytorch-ssd/vision/ssd/predictor.py\", line 4, in <module>\n",
      "    from .data_preprocessing import PredictionTransform\n",
      "  File \"/scratch/tor213/DLS-Homework5/pytorch-ssd/vision/ssd/data_preprocessing.py\", line 1, in <module>\n",
      "    from ..transforms.transforms import *\n",
      "  File \"/scratch/tor213/DLS-Homework5/pytorch-ssd/vision/transforms/transforms.py\", line 5, in <module>\n",
      "    from torchvision import transforms\n",
      "  File \"/home/tor213/.local/lib/python3.8/site-packages/torchvision/__init__.py\", line 7, in <module>\n",
      "    from torchvision import models\n",
      "  File \"/home/tor213/.local/lib/python3.8/site-packages/torchvision/models/__init__.py\", line 18, in <module>\n",
      "    from . import quantization\n",
      "  File \"/home/tor213/.local/lib/python3.8/site-packages/torchvision/models/quantization/__init__.py\", line 1, in <module>\n",
      "    from .mobilenet import *\n",
      "  File \"/home/tor213/.local/lib/python3.8/site-packages/torchvision/models/quantization/mobilenet.py\", line 1, in <module>\n",
      "    from .mobilenetv2 import QuantizableMobileNetV2, mobilenet_v2, __all__ as mv2_all\n",
      "  File \"/home/tor213/.local/lib/python3.8/site-packages/torchvision/models/quantization/mobilenetv2.py\", line 5, in <module>\n",
      "    from torch.ao.quantization import QuantStub, DeQuantStub\n",
      "ModuleNotFoundError: No module named 'torch.ao.quantization'\n"
     ]
    }
   ],
   "source": [
    "!python pytorch-ssd/eval_ssd.py --net mb1-ssd --dataset VOCdevkit/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U2ljHzGFZir"
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nWHrkZxE19h6",
    "outputId": "0ecb0b81-637e-4950-dfbe-e59b47de0ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.22.7)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.5.2)\n",
      "Requirement already satisfied: botocore<1.26.0,>=1.25.7 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.25.7)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.7->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.7->boto3) (1.26.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.7->boto3) (1.15.0)\n",
      "2022-05-05 07:43:18,864 - root - Download https://storage.googleapis.com/openimages/2018_04/class-descriptions-boxable.csv.\n",
      "2022-05-05 07:43:18,939 - root - Download https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv.\n",
      "2022-05-05 07:43:29,343 - root - Read annotation file open_images/train-annotations-bbox.csv\n",
      "2022-05-05 07:43:43,769 - root - train bounding boxes size: 24308\n",
      "2022-05-05 07:43:43,769 - root - Approximate Image Stats: \n",
      "2022-05-05 07:43:43,779 - root - Airplane: 12003/14401 = 0.83.\n",
      "2022-05-05 07:43:43,779 - root - Helicopter: 2398/14401 = 0.17.\n",
      "2022-05-05 07:43:43,780 - root - Label distribution: \n",
      "2022-05-05 07:43:43,781 - root - Airplane: 21285/24308 = 0.88.\n",
      "2022-05-05 07:43:43,781 - root - Helicopter: 3023/24308 = 0.12.\n",
      "2022-05-05 07:43:43,781 - root - Shuffle dataset.\n",
      "2022-05-05 07:43:43,781 - root - Save train data to open_images/sub-train-annotations-bbox.csv.\n",
      "2022-05-05 07:43:43,973 - root - Download https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv.\n",
      "2022-05-05 07:43:44,640 - root - Read annotation file open_images/validation-annotations-bbox.csv\n",
      "2022-05-05 07:43:44,928 - root - validation bounding boxes size: 1161\n",
      "2022-05-05 07:43:44,929 - root - Approximate Image Stats: \n",
      "2022-05-05 07:43:44,931 - root - Airplane: 753/878 = 0.86.\n",
      "2022-05-05 07:43:44,931 - root - Helicopter: 125/878 = 0.14.\n",
      "2022-05-05 07:43:44,931 - root - Label distribution: \n",
      "2022-05-05 07:43:44,931 - root - Airplane: 1027/1161 = 0.88.\n",
      "2022-05-05 07:43:44,931 - root - Helicopter: 134/1161 = 0.12.\n",
      "2022-05-05 07:43:44,931 - root - Shuffle dataset.\n",
      "2022-05-05 07:43:44,931 - root - Save validation data to open_images/sub-validation-annotations-bbox.csv.\n",
      "2022-05-05 07:43:44,947 - root - Download https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv.\n",
      "2022-05-05 07:43:45,636 - root - Read annotation file open_images/test-annotations-bbox.csv\n",
      "2022-05-05 07:43:46,796 - root - test bounding boxes size: 3696\n",
      "2022-05-05 07:43:46,796 - root - Approximate Image Stats: \n",
      "2022-05-05 07:43:46,800 - root - Airplane: 2434/2807 = 0.87.\n",
      "2022-05-05 07:43:46,800 - root - Helicopter: 373/2807 = 0.13.\n",
      "2022-05-05 07:43:46,800 - root - Label distribution: \n",
      "2022-05-05 07:43:46,800 - root - Airplane: 3272/3696 = 0.89.\n",
      "2022-05-05 07:43:46,800 - root - Helicopter: 424/3696 = 0.11.\n",
      "2022-05-05 07:43:46,800 - root - Shuffle dataset.\n",
      "2022-05-05 07:43:46,800 - root - Save test data to open_images/sub-test-annotations-bbox.csv.\n",
      "2022-05-05 07:43:46,823 - root - Start downloading 17877 images.\n",
      "2022-05-05 07:43:49,695 - root - Downloaded 100 images.\n",
      "2022-05-05 07:43:51,732 - root - Downloaded 200 images.\n",
      "2022-05-05 07:43:53,817 - root - Downloaded 300 images.\n",
      "2022-05-05 07:43:55,846 - root - Downloaded 400 images.\n",
      "2022-05-05 07:43:58,024 - root - Downloaded 500 images.\n",
      "2022-05-05 07:44:00,545 - root - Downloaded 600 images.\n",
      "2022-05-05 07:44:02,646 - root - Downloaded 700 images.\n",
      "2022-05-05 07:44:04,717 - root - Downloaded 800 images.\n",
      "2022-05-05 07:44:06,779 - root - Downloaded 900 images.\n",
      "2022-05-05 07:44:08,974 - root - Downloaded 1000 images.\n",
      "2022-05-05 07:44:11,342 - root - Downloaded 1100 images.\n",
      "2022-05-05 07:44:13,411 - root - Downloaded 1200 images.\n",
      "2022-05-05 07:44:15,559 - root - Downloaded 1300 images.\n",
      "2022-05-05 07:44:17,589 - root - Downloaded 1400 images.\n",
      "2022-05-05 07:44:19,879 - root - Downloaded 1500 images.\n",
      "2022-05-05 07:44:22,239 - root - Downloaded 1600 images.\n",
      "2022-05-05 07:44:24,336 - root - Downloaded 1700 images.\n",
      "2022-05-05 07:44:26,327 - root - Downloaded 1800 images.\n",
      "2022-05-05 07:44:28,420 - root - Downloaded 1900 images.\n",
      "2022-05-05 07:44:30,676 - root - Downloaded 2000 images.\n",
      "2022-05-05 07:44:33,033 - root - Downloaded 2100 images.\n",
      "2022-05-05 07:44:35,104 - root - Downloaded 2200 images.\n",
      "2022-05-05 07:44:37,169 - root - Downloaded 2300 images.\n",
      "2022-05-05 07:44:39,241 - root - Downloaded 2400 images.\n",
      "2022-05-05 07:44:41,641 - root - Downloaded 2500 images.\n",
      "2022-05-05 07:44:43,881 - root - Downloaded 2600 images.\n",
      "2022-05-05 07:44:45,998 - root - Downloaded 2700 images.\n",
      "2022-05-05 07:44:48,043 - root - Downloaded 2800 images.\n",
      "2022-05-05 07:44:50,145 - root - Downloaded 2900 images.\n",
      "2022-05-05 07:44:52,444 - root - Downloaded 3000 images.\n",
      "2022-05-05 07:44:54,802 - root - Downloaded 3100 images.\n",
      "2022-05-05 07:44:56,911 - root - Downloaded 3200 images.\n",
      "2022-05-05 07:44:58,931 - root - Downloaded 3300 images.\n",
      "2022-05-05 07:45:01,050 - root - Downloaded 3400 images.\n",
      "2022-05-05 07:45:03,453 - root - Downloaded 3500 images.\n",
      "2022-05-05 07:45:05,761 - root - Downloaded 3600 images.\n",
      "2022-05-05 07:45:07,772 - root - Downloaded 3700 images.\n",
      "2022-05-05 07:45:09,867 - root - Downloaded 3800 images.\n",
      "2022-05-05 07:45:11,974 - root - Downloaded 3900 images.\n",
      "2022-05-05 07:45:14,250 - root - Downloaded 4000 images.\n",
      "2022-05-05 07:45:16,609 - root - Downloaded 4100 images.\n",
      "2022-05-05 07:45:18,693 - root - Downloaded 4200 images.\n",
      "2022-05-05 07:45:20,730 - root - Downloaded 4300 images.\n",
      "2022-05-05 07:45:22,809 - root - Downloaded 4400 images.\n",
      "2022-05-05 07:45:25,126 - root - Downloaded 4500 images.\n",
      "2022-05-05 07:45:27,452 - root - Downloaded 4600 images.\n",
      "2022-05-05 07:45:29,553 - root - Downloaded 4700 images.\n",
      "2022-05-05 07:45:31,610 - root - Downloaded 4800 images.\n",
      "2022-05-05 07:45:33,695 - root - Downloaded 4900 images.\n",
      "2022-05-05 07:45:36,016 - root - Downloaded 5000 images.\n",
      "2022-05-05 07:45:38,261 - root - Downloaded 5100 images.\n",
      "2022-05-05 07:45:42,395 - root - Downloaded 5300 images.\n",
      "2022-05-05 07:45:44,459 - root - Downloaded 5400 images.\n",
      "2022-05-05 07:45:46,698 - root - Downloaded 5500 images.\n",
      "2022-05-05 07:45:49,019 - root - Downloaded 5600 images.\n",
      "2022-05-05 07:45:51,000 - root - Downloaded 5700 images.\n",
      "2022-05-05 07:45:53,102 - root - Downloaded 5800 images.\n",
      "2022-05-05 07:45:55,118 - root - Downloaded 5900 images.\n",
      "2022-05-05 07:45:57,393 - root - Downloaded 6000 images.\n",
      "2022-05-05 07:45:59,648 - root - Downloaded 6100 images.\n",
      "2022-05-05 07:46:01,819 - root - Downloaded 6200 images.\n",
      "2022-05-05 07:46:03,835 - root - Downloaded 6300 images.\n",
      "2022-05-05 07:46:05,881 - root - Downloaded 6400 images.\n",
      "2022-05-05 07:46:08,168 - root - Downloaded 6500 images.\n",
      "2022-05-05 07:46:10,474 - root - Downloaded 6600 images.\n",
      "2022-05-05 07:46:12,597 - root - Downloaded 6700 images.\n",
      "2022-05-05 07:46:14,657 - root - Downloaded 6800 images.\n",
      "2022-05-05 07:46:16,684 - root - Downloaded 6900 images.\n",
      "2022-05-05 07:46:18,990 - root - Downloaded 7000 images.\n",
      "2022-05-05 07:46:21,355 - root - Downloaded 7100 images.\n",
      "2022-05-05 07:46:23,401 - root - Downloaded 7200 images.\n",
      "2022-05-05 07:46:25,461 - root - Downloaded 7300 images.\n",
      "2022-05-05 07:46:27,531 - root - Downloaded 7400 images.\n",
      "2022-05-05 07:46:29,825 - root - Downloaded 7500 images.\n",
      "2022-05-05 07:46:32,238 - root - Downloaded 7600 images.\n",
      "2022-05-05 07:46:34,314 - root - Downloaded 7700 images.\n",
      "2022-05-05 07:46:36,394 - root - Downloaded 7800 images.\n",
      "2022-05-05 07:46:38,454 - root - Downloaded 7900 images.\n",
      "2022-05-05 07:46:40,936 - root - Downloaded 8000 images.\n",
      "2022-05-05 07:46:43,160 - root - Downloaded 8100 images.\n",
      "2022-05-05 07:46:45,203 - root - Downloaded 8200 images.\n",
      "2022-05-05 07:46:47,277 - root - Downloaded 8300 images.\n",
      "2022-05-05 07:46:49,354 - root - Downloaded 8400 images.\n",
      "2022-05-05 07:46:51,668 - root - Downloaded 8500 images.\n",
      "2022-05-05 07:46:53,981 - root - Downloaded 8600 images.\n",
      "2022-05-05 07:46:56,063 - root - Downloaded 8700 images.\n",
      "2022-05-05 07:46:58,166 - root - Downloaded 8800 images.\n",
      "2022-05-05 07:47:00,219 - root - Downloaded 8900 images.\n",
      "2022-05-05 07:47:02,574 - root - Downloaded 9000 images.\n",
      "2022-05-05 07:47:04,788 - root - Downloaded 9100 images.\n",
      "2022-05-05 07:47:06,900 - root - Downloaded 9200 images.\n",
      "2022-05-05 07:47:08,944 - root - Downloaded 9300 images.\n",
      "2022-05-05 07:47:11,105 - root - Downloaded 9400 images.\n",
      "2022-05-05 07:47:13,431 - root - Downloaded 9500 images.\n",
      "2022-05-05 07:47:15,781 - root - Downloaded 9600 images.\n",
      "2022-05-05 07:47:17,873 - root - Downloaded 9700 images.\n",
      "2022-05-05 07:47:19,939 - root - Downloaded 9800 images.\n",
      "2022-05-05 07:47:21,976 - root - Downloaded 9900 images.\n",
      "2022-05-05 07:47:24,279 - root - Downloaded 10000 images.\n",
      "2022-05-05 07:47:26,613 - root - Downloaded 10100 images.\n",
      "2022-05-05 07:47:28,741 - root - Downloaded 10200 images.\n",
      "2022-05-05 07:47:30,756 - root - Downloaded 10300 images.\n",
      "2022-05-05 07:47:32,831 - root - Downloaded 10400 images.\n",
      "2022-05-05 07:47:35,198 - root - Downloaded 10500 images.\n",
      "2022-05-05 07:47:37,540 - root - Downloaded 10600 images.\n",
      "2022-05-05 07:47:39,578 - root - Downloaded 10700 images.\n",
      "2022-05-05 07:47:41,674 - root - Downloaded 10800 images.\n",
      "2022-05-05 07:47:43,794 - root - Downloaded 10900 images.\n",
      "2022-05-05 07:47:46,190 - root - Downloaded 11000 images.\n",
      "2022-05-05 07:47:48,555 - root - Downloaded 11100 images.\n",
      "2022-05-05 07:47:50,653 - root - Downloaded 11200 images.\n",
      "2022-05-05 07:47:52,768 - root - Downloaded 11300 images.\n",
      "2022-05-05 07:47:54,810 - root - Downloaded 11400 images.\n",
      "2022-05-05 07:47:57,231 - root - Downloaded 11500 images.\n",
      "2022-05-05 07:47:59,553 - root - Downloaded 11600 images.\n",
      "2022-05-05 07:48:01,627 - root - Downloaded 11700 images.\n",
      "2022-05-05 07:48:03,690 - root - Downloaded 11800 images.\n",
      "2022-05-05 07:48:05,799 - root - Downloaded 11900 images.\n",
      "2022-05-05 07:48:08,130 - root - Downloaded 12000 images.\n",
      "2022-05-05 07:48:10,355 - root - Downloaded 12100 images.\n",
      "2022-05-05 07:48:12,424 - root - Downloaded 12200 images.\n",
      "2022-05-05 07:48:14,498 - root - Downloaded 12300 images.\n",
      "2022-05-05 07:48:16,532 - root - Downloaded 12400 images.\n",
      "2022-05-05 07:48:18,851 - root - Downloaded 12500 images.\n",
      "2022-05-05 07:48:21,112 - root - Downloaded 12600 images.\n",
      "2022-05-05 07:48:23,192 - root - Downloaded 12700 images.\n",
      "2022-05-05 07:48:25,330 - root - Downloaded 12800 images.\n",
      "2022-05-05 07:48:27,354 - root - Downloaded 12900 images.\n",
      "2022-05-05 07:48:29,761 - root - Downloaded 13000 images.\n",
      "2022-05-05 07:48:32,094 - root - Downloaded 13100 images.\n",
      "2022-05-05 07:48:34,155 - root - Downloaded 13200 images.\n",
      "2022-05-05 07:48:36,248 - root - Downloaded 13300 images.\n",
      "2022-05-05 07:48:38,326 - root - Downloaded 13400 images.\n",
      "2022-05-05 07:48:40,702 - root - Downloaded 13500 images.\n",
      "2022-05-05 07:48:43,131 - root - Downloaded 13600 images.\n",
      "2022-05-05 07:48:45,227 - root - Downloaded 13700 images.\n",
      "2022-05-05 07:48:47,358 - root - Downloaded 13800 images.\n",
      "2022-05-05 07:48:49,418 - root - Downloaded 13900 images.\n",
      "2022-05-05 07:48:51,793 - root - Downloaded 14000 images.\n",
      "2022-05-05 07:48:54,136 - root - Downloaded 14100 images.\n",
      "2022-05-05 07:48:56,221 - root - Downloaded 14200 images.\n",
      "2022-05-05 07:48:58,309 - root - Downloaded 14300 images.\n",
      "2022-05-05 07:49:00,468 - root - Downloaded 14400 images.\n",
      "2022-05-05 07:49:02,705 - root - Downloaded 14500 images.\n",
      "2022-05-05 07:49:05,047 - root - Downloaded 14600 images.\n",
      "2022-05-05 07:49:07,074 - root - Downloaded 14700 images.\n",
      "2022-05-05 07:49:09,165 - root - Downloaded 14800 images.\n",
      "2022-05-05 07:49:11,293 - root - Downloaded 14900 images.\n",
      "2022-05-05 07:49:13,649 - root - Downloaded 15000 images.\n",
      "2022-05-05 07:49:16,034 - root - Downloaded 15100 images.\n",
      "2022-05-05 07:49:18,174 - root - Downloaded 15200 images.\n",
      "2022-05-05 07:49:20,308 - root - Downloaded 15300 images.\n",
      "2022-05-05 07:49:22,416 - root - Downloaded 15400 images.\n",
      "2022-05-05 07:49:24,641 - root - Downloaded 15500 images.\n",
      "2022-05-05 07:49:27,064 - root - Downloaded 15600 images.\n",
      "2022-05-05 07:49:29,159 - root - Downloaded 15700 images.\n",
      "2022-05-05 07:49:31,595 - root - Downloaded 15800 images.\n",
      "2022-05-05 07:49:33,620 - root - Downloaded 15900 images.\n",
      "2022-05-05 07:49:35,882 - root - Downloaded 16000 images.\n",
      "2022-05-05 07:49:38,295 - root - Downloaded 16100 images.\n",
      "2022-05-05 07:49:40,445 - root - Downloaded 16200 images.\n",
      "2022-05-05 07:49:42,614 - root - Downloaded 16300 images.\n",
      "2022-05-05 07:49:44,782 - root - Downloaded 16400 images.\n",
      "2022-05-05 07:49:47,010 - root - Downloaded 16500 images.\n",
      "2022-05-05 07:49:49,386 - root - Downloaded 16600 images.\n",
      "2022-05-05 07:49:51,523 - root - Downloaded 16700 images.\n",
      "2022-05-05 07:49:53,692 - root - Downloaded 16800 images.\n",
      "2022-05-05 07:49:55,801 - root - Downloaded 16900 images.\n",
      "2022-05-05 07:49:57,980 - root - Downloaded 17000 images.\n",
      "2022-05-05 07:50:00,444 - root - Downloaded 17100 images.\n",
      "2022-05-05 07:50:02,583 - root - Downloaded 17200 images.\n",
      "2022-05-05 07:50:04,744 - root - Downloaded 17300 images.\n",
      "2022-05-05 07:50:06,881 - root - Downloaded 17400 images.\n",
      "2022-05-05 07:50:09,018 - root - Downloaded 17500 images.\n",
      "2022-05-05 07:50:11,451 - root - Downloaded 17600 images.\n",
      "2022-05-05 07:50:13,534 - root - Downloaded 17700 images.\n",
      "2022-05-05 07:50:15,838 - root - Downloaded 17800 images.\n",
      "2022-05-05 07:50:18,425 - root - Task Done.\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "!python3 pytorch-ssd/open_images_downloader.py --root open_images --class_names \"Airplane, Helicopter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toXR2u08TWgT"
   },
   "source": [
    "### Training SSD on Airplanes and Helicopters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4yqJJA_19h7",
    "outputId": "b5939366-2c03-4175-9880-c0668b830aa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "2022-05-05 07:52:45,510 - root - INFO - Use Cuda.\n",
      "2022-05-05 07:52:45,510 - root - INFO - Namespace(balance_data=False, base_net=None, base_net_lr=0.001, batch_size=5, checkpoint_folder='models/', dataset_type='open_images', datasets=['open_images'], debug_steps=100, extra_layers_lr=None, freeze_base_net=False, freeze_net=False, gamma=0.1, lr=0.01, mb2_width_mult=1.0, milestones='80,100', momentum=0.9, net='mb1-ssd', num_epochs=100, num_workers=4, pretrained_ssd='models/mobilenet-v1-ssd-mp-0_675.pth', resume=None, scheduler='cosine', t_max=100.0, use_cuda=True, validation_dataset=None, validation_epochs=5, weight_decay=0.0005)\n",
      "2022-05-05 07:52:45,511 - root - INFO - Prepare training datasets.\n",
      "2022-05-05 07:52:52,371 - root - INFO - Dataset Summary:Number of Images: 14228\n",
      "Minimum Number of Images for a Class: -1\n",
      "Label Distribution:\n",
      "\tAirplane: 21285\n",
      "\tHelicopter: 3023\n",
      "2022-05-05 07:52:52,386 - root - INFO - Stored labels into file models/open-images-model-labels.txt.\n",
      "2022-05-05 07:52:52,386 - root - INFO - Train dataset size: 14228\n",
      "2022-05-05 07:52:52,386 - root - INFO - Prepare Validation datasets.\n",
      "2022-05-05 07:52:53,717 - root - INFO - Dataset Summary:Number of Images: 2780\n",
      "Minimum Number of Images for a Class: -1\n",
      "Label Distribution:\n",
      "\tAirplane: 3272\n",
      "\tHelicopter: 424\n",
      "2022-05-05 07:52:53,720 - root - INFO - validation dataset size: 2780\n",
      "2022-05-05 07:52:53,720 - root - INFO - Build network.\n",
      "2022-05-05 07:52:53,784 - root - INFO - Init from pretrained ssd models/mobilenet-v1-ssd-mp-0_675.pth\n",
      "2022-05-05 07:52:53,829 - root - INFO - Took 0.04 seconds to load the model.\n",
      "2022-05-05 07:52:57,594 - root - INFO - Learning rate: 0.01, Base net learning rate: 0.001, Extra Layers learning rate: 0.01.\n",
      "2022-05-05 07:52:57,594 - root - INFO - Uses CosineAnnealingLR scheduler.\n",
      "2022-05-05 07:52:57,594 - root - INFO - Start training from epoch 0.\n",
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "2022-05-05 07:53:14,297 - root - INFO - Epoch: 0, Step: 100, Average Loss: 5.6512, Average Regression Loss 2.3195, Average Classification Loss: 3.3318\n",
      "2022-05-05 07:53:28,810 - root - INFO - Epoch: 0, Step: 200, Average Loss: 4.0970, Average Regression Loss 1.6069, Average Classification Loss: 2.4901\n",
      "tcmalloc: large alloc 1810251776 bytes == 0xdf230000 @  0x7f0017d8c001 0x7eff5baa51af 0x7eff5bafbc23 0x7eff5bafca87 0x7eff5bb9e823 0x5936cc 0x548c51 0x5127f1 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x4bcb19 0x532e76 0x594b72 0x548cc1 0x51566f 0x4bc98a 0x532e76 0x594b72 0x548cc1 0x51566f 0x593dd7 0x511e2c 0x4bc98a 0x533274 0x4d3969 0x512147 0x4bc98a 0x533274\n",
      "2022-05-05 07:53:47,728 - root - INFO - Epoch: 0, Step: 300, Average Loss: 3.9402, Average Regression Loss 1.4618, Average Classification Loss: 2.4785\n",
      "2022-05-05 07:54:03,253 - root - INFO - Epoch: 0, Step: 400, Average Loss: 3.7599, Average Regression Loss 1.3389, Average Classification Loss: 2.4210\n",
      "2022-05-05 07:54:19,021 - root - INFO - Epoch: 0, Step: 500, Average Loss: 3.8774, Average Regression Loss 1.4013, Average Classification Loss: 2.4761\n",
      "2022-05-05 07:54:34,553 - root - INFO - Epoch: 0, Step: 600, Average Loss: 3.7548, Average Regression Loss 1.4017, Average Classification Loss: 2.3531\n",
      "2022-05-05 07:54:51,090 - root - INFO - Epoch: 0, Step: 700, Average Loss: 3.7942, Average Regression Loss 1.3804, Average Classification Loss: 2.4137\n",
      "2022-05-05 07:55:07,793 - root - INFO - Epoch: 0, Step: 800, Average Loss: 3.6994, Average Regression Loss 1.3278, Average Classification Loss: 2.3716\n",
      "2022-05-05 07:55:23,311 - root - INFO - Epoch: 0, Step: 900, Average Loss: 3.5960, Average Regression Loss 1.2788, Average Classification Loss: 2.3172\n",
      "2022-05-05 07:55:39,214 - root - INFO - Epoch: 0, Step: 1000, Average Loss: 3.5334, Average Regression Loss 1.3058, Average Classification Loss: 2.2277\n",
      "2022-05-05 07:55:53,778 - root - INFO - Epoch: 0, Step: 1100, Average Loss: 3.6805, Average Regression Loss 1.3755, Average Classification Loss: 2.3050\n",
      "2022-05-05 07:56:08,825 - root - INFO - Epoch: 0, Step: 1200, Average Loss: 3.5439, Average Regression Loss 1.2898, Average Classification Loss: 2.2542\n",
      "2022-05-05 07:56:24,044 - root - INFO - Epoch: 0, Step: 1300, Average Loss: 3.5094, Average Regression Loss 1.2202, Average Classification Loss: 2.2893\n",
      "2022-05-05 07:56:39,355 - root - INFO - Epoch: 0, Step: 1400, Average Loss: 3.3295, Average Regression Loss 1.1248, Average Classification Loss: 2.2047\n",
      "2022-05-05 07:56:54,107 - root - INFO - Epoch: 0, Step: 1500, Average Loss: 3.4272, Average Regression Loss 1.2485, Average Classification Loss: 2.1787\n",
      "2022-05-05 07:57:10,686 - root - INFO - Epoch: 0, Step: 1600, Average Loss: 3.4204, Average Regression Loss 1.2389, Average Classification Loss: 2.1816\n",
      "2022-05-05 07:57:26,930 - root - INFO - Epoch: 0, Step: 1700, Average Loss: 3.3414, Average Regression Loss 1.1525, Average Classification Loss: 2.1888\n",
      "2022-05-05 07:57:42,729 - root - INFO - Epoch: 0, Step: 1800, Average Loss: 3.3569, Average Regression Loss 1.1745, Average Classification Loss: 2.1824\n",
      "2022-05-05 07:57:58,436 - root - INFO - Epoch: 0, Step: 1900, Average Loss: 3.3322, Average Regression Loss 1.1991, Average Classification Loss: 2.1331\n",
      "2022-05-05 07:58:13,828 - root - INFO - Epoch: 0, Step: 2000, Average Loss: 3.3628, Average Regression Loss 1.2033, Average Classification Loss: 2.1596\n",
      "2022-05-05 07:58:29,349 - root - INFO - Epoch: 0, Step: 2100, Average Loss: 3.3959, Average Regression Loss 1.2191, Average Classification Loss: 2.1768\n",
      "2022-05-05 07:58:44,732 - root - INFO - Epoch: 0, Step: 2200, Average Loss: 3.3195, Average Regression Loss 1.1653, Average Classification Loss: 2.1541\n",
      "2022-05-05 07:59:00,028 - root - INFO - Epoch: 0, Step: 2300, Average Loss: 3.1374, Average Regression Loss 1.0858, Average Classification Loss: 2.0516\n",
      "2022-05-05 07:59:15,379 - root - INFO - Epoch: 0, Step: 2400, Average Loss: 3.4290, Average Regression Loss 1.2185, Average Classification Loss: 2.2105\n",
      "2022-05-05 07:59:30,962 - root - INFO - Epoch: 0, Step: 2500, Average Loss: 3.2490, Average Regression Loss 1.1326, Average Classification Loss: 2.1163\n",
      "2022-05-05 07:59:47,182 - root - INFO - Epoch: 0, Step: 2600, Average Loss: 3.2686, Average Regression Loss 1.1275, Average Classification Loss: 2.1411\n",
      "2022-05-05 08:00:02,516 - root - INFO - Epoch: 0, Step: 2700, Average Loss: 3.3478, Average Regression Loss 1.1704, Average Classification Loss: 2.1774\n",
      "2022-05-05 08:00:18,064 - root - INFO - Epoch: 0, Step: 2800, Average Loss: 3.2379, Average Regression Loss 1.1440, Average Classification Loss: 2.0940\n",
      "2022-05-05 08:00:48,223 - root - INFO - Epoch: 0, Validation Loss: 2.2466, Validation Regression Loss 0.7048, Validation Classification Loss: 1.5418\n",
      "2022-05-05 08:00:48,290 - root - INFO - Saved model models/mb1-ssd-Epoch-0-Loss-2.2465531746689362.pth\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "tcmalloc: large alloc 2097692672 bytes == 0xdf230000 @  0x7f0017d8c001 0x7eff5baa51af 0x7eff5bafbc23 0x7eff5bafca87 0x7eff5bb9e823 0x5936cc 0x548c51 0x5127f1 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x4bcb19 0x532e76 0x594b72 0x548cc1 0x51566f 0x4bc98a 0x532e76 0x594b72 0x548cc1 0x51566f 0x593dd7 0x511e2c 0x4bc98a 0x533274 0x4d3969 0x512147 0x4bc98a 0x533274\n",
      "tcmalloc: large alloc 1306140672 bytes == 0xbffa4000 @  0x7f0017d8c001 0x7eff5baa51af 0x7eff5bafbc23 0x7eff5bafca87 0x7eff5bb9e823 0x5936cc 0x548c51 0x5127f1 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x4bcb19 0x532e76 0x594b72 0x548cc1 0x51566f 0x4bc98a 0x532e76 0x594b72 0x548cc1 0x51566f 0x593dd7 0x511e2c 0x4bc98a 0x533274 0x4d3969 0x512147 0x4bc98a 0x533274\n",
      "2022-05-05 08:01:08,728 - root - INFO - Epoch: 1, Step: 100, Average Loss: 3.2263, Average Regression Loss 1.1141, Average Classification Loss: 2.1123\n",
      "2022-05-05 08:01:24,092 - root - INFO - Epoch: 1, Step: 200, Average Loss: 3.2312, Average Regression Loss 1.1136, Average Classification Loss: 2.1175\n",
      "2022-05-05 08:01:39,830 - root - INFO - Epoch: 1, Step: 300, Average Loss: 3.1898, Average Regression Loss 1.1172, Average Classification Loss: 2.0726\n",
      "2022-05-05 08:01:55,295 - root - INFO - Epoch: 1, Step: 400, Average Loss: 3.2016, Average Regression Loss 1.1305, Average Classification Loss: 2.0711\n",
      "2022-05-05 08:02:10,281 - root - INFO - Epoch: 1, Step: 500, Average Loss: 3.1122, Average Regression Loss 1.0590, Average Classification Loss: 2.0532\n",
      "2022-05-05 08:02:25,139 - root - INFO - Epoch: 1, Step: 600, Average Loss: 3.1274, Average Regression Loss 1.0829, Average Classification Loss: 2.0445\n",
      "2022-05-05 08:02:40,812 - root - INFO - Epoch: 1, Step: 700, Average Loss: 3.0170, Average Regression Loss 1.0485, Average Classification Loss: 1.9684\n",
      "2022-05-05 08:02:56,569 - root - INFO - Epoch: 1, Step: 800, Average Loss: 2.9796, Average Regression Loss 1.0040, Average Classification Loss: 1.9756\n",
      "2022-05-05 08:03:11,027 - root - INFO - Epoch: 1, Step: 900, Average Loss: 3.2350, Average Regression Loss 1.1558, Average Classification Loss: 2.0792\n",
      "2022-05-05 08:03:26,331 - root - INFO - Epoch: 1, Step: 1000, Average Loss: 3.0825, Average Regression Loss 1.0521, Average Classification Loss: 2.0304\n",
      "2022-05-05 08:03:41,590 - root - INFO - Epoch: 1, Step: 1100, Average Loss: 3.1524, Average Regression Loss 1.0969, Average Classification Loss: 2.0555\n",
      "2022-05-05 08:03:56,924 - root - INFO - Epoch: 1, Step: 1200, Average Loss: 3.1061, Average Regression Loss 1.0720, Average Classification Loss: 2.0342\n",
      "2022-05-05 08:04:12,273 - root - INFO - Epoch: 1, Step: 1300, Average Loss: 3.0667, Average Regression Loss 1.0575, Average Classification Loss: 2.0092\n",
      "2022-05-05 08:04:27,385 - root - INFO - Epoch: 1, Step: 1400, Average Loss: 2.9909, Average Regression Loss 1.0147, Average Classification Loss: 1.9762\n",
      "2022-05-05 08:04:43,283 - root - INFO - Epoch: 1, Step: 1500, Average Loss: 3.1426, Average Regression Loss 1.0717, Average Classification Loss: 2.0708\n",
      "2022-05-05 08:04:59,140 - root - INFO - Epoch: 1, Step: 1600, Average Loss: 3.3263, Average Regression Loss 1.2366, Average Classification Loss: 2.0898\n",
      "2022-05-05 08:05:14,587 - root - INFO - Epoch: 1, Step: 1700, Average Loss: 3.3330, Average Regression Loss 1.2408, Average Classification Loss: 2.0922\n",
      "2022-05-05 08:05:30,362 - root - INFO - Epoch: 1, Step: 1800, Average Loss: 3.1199, Average Regression Loss 1.0470, Average Classification Loss: 2.0729\n",
      "2022-05-05 08:05:46,006 - root - INFO - Epoch: 1, Step: 1900, Average Loss: 3.1899, Average Regression Loss 1.0831, Average Classification Loss: 2.1068\n",
      "2022-05-05 08:06:01,857 - root - INFO - Epoch: 1, Step: 2000, Average Loss: 3.1976, Average Regression Loss 1.0640, Average Classification Loss: 2.1336\n",
      "2022-05-05 08:06:16,844 - root - INFO - Epoch: 1, Step: 2100, Average Loss: 2.9731, Average Regression Loss 1.0025, Average Classification Loss: 1.9706\n",
      "2022-05-05 08:06:32,847 - root - INFO - Epoch: 1, Step: 2200, Average Loss: 3.0767, Average Regression Loss 1.0578, Average Classification Loss: 2.0189\n",
      "2022-05-05 08:06:48,107 - root - INFO - Epoch: 1, Step: 2300, Average Loss: 3.1837, Average Regression Loss 1.1065, Average Classification Loss: 2.0772\n",
      "2022-05-05 08:07:03,547 - root - INFO - Epoch: 1, Step: 2400, Average Loss: 3.2003, Average Regression Loss 1.1320, Average Classification Loss: 2.0683\n",
      "2022-05-05 08:07:18,274 - root - INFO - Epoch: 1, Step: 2500, Average Loss: 3.0879, Average Regression Loss 1.0557, Average Classification Loss: 2.0322\n",
      "2022-05-05 08:07:34,265 - root - INFO - Epoch: 1, Step: 2600, Average Loss: 3.0046, Average Regression Loss 1.0348, Average Classification Loss: 1.9698\n",
      "2022-05-05 08:07:49,988 - root - INFO - Epoch: 1, Step: 2700, Average Loss: 3.1473, Average Regression Loss 1.0544, Average Classification Loss: 2.0929\n",
      "2022-05-05 08:08:04,813 - root - INFO - Epoch: 1, Step: 2800, Average Loss: 3.1616, Average Regression Loss 1.1270, Average Classification Loss: 2.0346\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "2022-05-05 08:08:27,228 - root - INFO - Epoch: 2, Step: 100, Average Loss: 2.9732, Average Regression Loss 1.0427, Average Classification Loss: 1.9305\n",
      "2022-05-05 08:08:41,964 - root - INFO - Epoch: 2, Step: 200, Average Loss: 2.9853, Average Regression Loss 1.0488, Average Classification Loss: 1.9365\n",
      "2022-05-05 08:08:57,375 - root - INFO - Epoch: 2, Step: 300, Average Loss: 3.2359, Average Regression Loss 1.1452, Average Classification Loss: 2.0907\n",
      "2022-05-05 08:09:13,226 - root - INFO - Epoch: 2, Step: 400, Average Loss: 3.0518, Average Regression Loss 1.0407, Average Classification Loss: 2.0111\n",
      "2022-05-05 08:09:27,791 - root - INFO - Epoch: 2, Step: 500, Average Loss: 3.1082, Average Regression Loss 1.1189, Average Classification Loss: 1.9893\n",
      "2022-05-05 08:09:43,225 - root - INFO - Epoch: 2, Step: 600, Average Loss: 3.0027, Average Regression Loss 1.0550, Average Classification Loss: 1.9477\n",
      "2022-05-05 08:09:57,942 - root - INFO - Epoch: 2, Step: 700, Average Loss: 2.9157, Average Regression Loss 0.9748, Average Classification Loss: 1.9409\n",
      "2022-05-05 08:10:13,509 - root - INFO - Epoch: 2, Step: 800, Average Loss: 3.1938, Average Regression Loss 1.1410, Average Classification Loss: 2.0528\n",
      "2022-05-05 08:10:29,455 - root - INFO - Epoch: 2, Step: 900, Average Loss: 3.0352, Average Regression Loss 1.0703, Average Classification Loss: 1.9649\n",
      "2022-05-05 08:10:44,926 - root - INFO - Epoch: 2, Step: 1000, Average Loss: 3.0279, Average Regression Loss 1.0361, Average Classification Loss: 1.9918\n",
      "2022-05-05 08:11:00,841 - root - INFO - Epoch: 2, Step: 1100, Average Loss: 2.9742, Average Regression Loss 1.0362, Average Classification Loss: 1.9380\n",
      "2022-05-05 08:11:18,236 - root - INFO - Epoch: 2, Step: 1200, Average Loss: 3.2288, Average Regression Loss 1.1775, Average Classification Loss: 2.0512\n",
      "tcmalloc: large alloc 1932132352 bytes == 0xe4ff4000 @  0x7f0017d8c001 0x7eff5baa51af 0x7eff5bafbc23 0x7eff5bafca87 0x7eff5bb9e823 0x5936cc 0x548c51 0x5127f1 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x4bcb19 0x532e76 0x594b72 0x548cc1 0x51566f 0x4bc98a 0x532e76 0x594b72 0x548cc1 0x51566f 0x593dd7 0x511e2c 0x4bc98a 0x533274 0x4d3969 0x512147 0x4bc98a 0x533274\n",
      "tcmalloc: large alloc 1648107520 bytes == 0x158dc8000 @  0x7f0017d8a1e7 0x7eff5baa50ce 0x7eff5baff726 0x7eff5baf2475 0x7eff4792ec30 0x7eff47bbd194 0x593784 0x548c51 0x51566f 0x549576 0x4bcb19 0x532e76 0x594b72 0x515600 0x549576 0x4bcb19 0x532e76 0x594b72 0x548cc1 0x51566f 0x4bc98a 0x532e76 0x594b72 0x548cc1 0x51566f 0x593dd7 0x511e2c 0x4bc98a 0x533274 0x4d3969 0x512147\n",
      "2022-05-05 08:11:36,702 - root - INFO - Epoch: 2, Step: 1300, Average Loss: 3.0223, Average Regression Loss 1.0362, Average Classification Loss: 1.9860\n",
      "2022-05-05 08:11:52,404 - root - INFO - Epoch: 2, Step: 1400, Average Loss: 3.1192, Average Regression Loss 1.0835, Average Classification Loss: 2.0356\n",
      "2022-05-05 08:12:07,888 - root - INFO - Epoch: 2, Step: 1500, Average Loss: 2.9071, Average Regression Loss 0.9760, Average Classification Loss: 1.9312\n",
      "2022-05-05 08:12:22,510 - root - INFO - Epoch: 2, Step: 1600, Average Loss: 2.9925, Average Regression Loss 1.0298, Average Classification Loss: 1.9627\n",
      "2022-05-05 08:12:38,275 - root - INFO - Epoch: 2, Step: 1700, Average Loss: 2.9943, Average Regression Loss 1.0318, Average Classification Loss: 1.9625\n",
      "2022-05-05 08:12:55,008 - root - INFO - Epoch: 2, Step: 1800, Average Loss: 2.8762, Average Regression Loss 0.9761, Average Classification Loss: 1.9000\n",
      "2022-05-05 08:13:09,483 - root - INFO - Epoch: 2, Step: 1900, Average Loss: 3.0152, Average Regression Loss 0.9991, Average Classification Loss: 2.0160\n",
      "2022-05-05 08:13:25,603 - root - INFO - Epoch: 2, Step: 2000, Average Loss: 2.9092, Average Regression Loss 0.9787, Average Classification Loss: 1.9305\n",
      "2022-05-05 08:13:41,769 - root - INFO - Epoch: 2, Step: 2100, Average Loss: 3.0578, Average Regression Loss 1.0894, Average Classification Loss: 1.9684\n",
      "2022-05-05 08:13:58,203 - root - INFO - Epoch: 2, Step: 2200, Average Loss: 3.0224, Average Regression Loss 1.0180, Average Classification Loss: 2.0044\n",
      "2022-05-05 08:14:14,615 - root - INFO - Epoch: 2, Step: 2300, Average Loss: 2.9664, Average Regression Loss 1.0125, Average Classification Loss: 1.9540\n",
      "tcmalloc: large alloc 1803239424 bytes == 0xb4964000 @  0x7f0017d8c001 0x7eff5baa51af 0x7eff5bafbc23 0x7eff5bafca87 0x7eff5bb9e823 0x5936cc 0x548c51 0x5127f1 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x4bcb19 0x532e76 0x594b72 0x548cc1 0x51566f 0x4bc98a 0x532e76 0x594b72 0x548cc1 0x51566f 0x593dd7 0x511e2c 0x4bc98a 0x533274 0x4d3969 0x512147 0x4bc98a 0x533274\n",
      "2022-05-05 08:14:33,161 - root - INFO - Epoch: 2, Step: 2400, Average Loss: 3.1891, Average Regression Loss 1.1373, Average Classification Loss: 2.0518\n",
      "2022-05-05 08:14:48,277 - root - INFO - Epoch: 2, Step: 2500, Average Loss: 2.8720, Average Regression Loss 0.9608, Average Classification Loss: 1.9112\n",
      "2022-05-05 08:15:04,574 - root - INFO - Epoch: 2, Step: 2600, Average Loss: 2.9499, Average Regression Loss 0.9782, Average Classification Loss: 1.9717\n",
      "2022-05-05 08:15:19,410 - root - INFO - Epoch: 2, Step: 2700, Average Loss: 3.0356, Average Regression Loss 1.0919, Average Classification Loss: 1.9437\n",
      "2022-05-05 08:15:35,410 - root - INFO - Epoch: 2, Step: 2800, Average Loss: 2.9066, Average Regression Loss 1.0260, Average Classification Loss: 1.8807\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "2022-05-05 08:15:59,362 - root - INFO - Epoch: 3, Step: 100, Average Loss: 3.0179, Average Regression Loss 1.0472, Average Classification Loss: 1.9707\n",
      "2022-05-05 08:16:15,406 - root - INFO - Epoch: 3, Step: 200, Average Loss: 2.9505, Average Regression Loss 1.0637, Average Classification Loss: 1.8868\n",
      "2022-05-05 08:16:30,243 - root - INFO - Epoch: 3, Step: 300, Average Loss: 2.9233, Average Regression Loss 0.9919, Average Classification Loss: 1.9314\n",
      "2022-05-05 08:16:46,224 - root - INFO - Epoch: 3, Step: 400, Average Loss: 3.1581, Average Regression Loss 1.1150, Average Classification Loss: 2.0430\n",
      "2022-05-05 08:17:00,544 - root - INFO - Epoch: 3, Step: 500, Average Loss: 2.9602, Average Regression Loss 0.9816, Average Classification Loss: 1.9786\n",
      "2022-05-05 08:17:15,487 - root - INFO - Epoch: 3, Step: 600, Average Loss: 2.8443, Average Regression Loss 0.9426, Average Classification Loss: 1.9017\n",
      "2022-05-05 08:17:31,190 - root - INFO - Epoch: 3, Step: 700, Average Loss: 2.9195, Average Regression Loss 1.0252, Average Classification Loss: 1.8942\n",
      "2022-05-05 08:17:47,125 - root - INFO - Epoch: 3, Step: 800, Average Loss: 3.0546, Average Regression Loss 1.0763, Average Classification Loss: 1.9784\n",
      "2022-05-05 08:18:02,318 - root - INFO - Epoch: 3, Step: 900, Average Loss: 2.9577, Average Regression Loss 0.9952, Average Classification Loss: 1.9625\n",
      "2022-05-05 08:18:18,282 - root - INFO - Epoch: 3, Step: 1000, Average Loss: 2.9287, Average Regression Loss 1.0050, Average Classification Loss: 1.9237\n",
      "2022-05-05 08:18:33,859 - root - INFO - Epoch: 3, Step: 1100, Average Loss: 3.0134, Average Regression Loss 1.0269, Average Classification Loss: 1.9865\n",
      "2022-05-05 08:18:49,455 - root - INFO - Epoch: 3, Step: 1200, Average Loss: 2.9456, Average Regression Loss 0.9922, Average Classification Loss: 1.9534\n",
      "2022-05-05 08:19:05,285 - root - INFO - Epoch: 3, Step: 1300, Average Loss: 3.0328, Average Regression Loss 1.0973, Average Classification Loss: 1.9355\n",
      "2022-05-05 08:19:20,089 - root - INFO - Epoch: 3, Step: 1400, Average Loss: 2.9754, Average Regression Loss 1.0311, Average Classification Loss: 1.9443\n",
      "2022-05-05 08:19:35,530 - root - INFO - Epoch: 3, Step: 1500, Average Loss: 3.0555, Average Regression Loss 1.0715, Average Classification Loss: 1.9840\n",
      "2022-05-05 08:19:50,724 - root - INFO - Epoch: 3, Step: 1600, Average Loss: 3.0650, Average Regression Loss 1.0371, Average Classification Loss: 2.0279\n",
      "2022-05-05 08:20:06,113 - root - INFO - Epoch: 3, Step: 1700, Average Loss: 2.9897, Average Regression Loss 1.0777, Average Classification Loss: 1.9119\n",
      "2022-05-05 08:20:21,425 - root - INFO - Epoch: 3, Step: 1800, Average Loss: 2.8307, Average Regression Loss 0.9264, Average Classification Loss: 1.9043\n",
      "2022-05-05 08:20:36,606 - root - INFO - Epoch: 3, Step: 1900, Average Loss: 2.9002, Average Regression Loss 0.9672, Average Classification Loss: 1.9330\n",
      "2022-05-05 08:20:53,528 - root - INFO - Epoch: 3, Step: 2000, Average Loss: 3.0003, Average Regression Loss 1.0297, Average Classification Loss: 1.9705\n",
      "2022-05-05 08:21:09,472 - root - INFO - Epoch: 3, Step: 2100, Average Loss: 2.8620, Average Regression Loss 0.9899, Average Classification Loss: 1.8721\n",
      "2022-05-05 08:21:25,899 - root - INFO - Epoch: 3, Step: 2200, Average Loss: 2.8609, Average Regression Loss 1.0226, Average Classification Loss: 1.8383\n",
      "2022-05-05 08:21:41,472 - root - INFO - Epoch: 3, Step: 2300, Average Loss: 2.9009, Average Regression Loss 0.9810, Average Classification Loss: 1.9199\n",
      "2022-05-05 08:21:58,514 - root - INFO - Epoch: 3, Step: 2400, Average Loss: 3.0471, Average Regression Loss 1.0312, Average Classification Loss: 2.0159\n",
      "2022-05-05 08:22:14,175 - root - INFO - Epoch: 3, Step: 2500, Average Loss: 2.8625, Average Regression Loss 0.9860, Average Classification Loss: 1.8765\n",
      "2022-05-05 08:22:30,776 - root - INFO - Epoch: 3, Step: 2600, Average Loss: 2.9670, Average Regression Loss 1.0194, Average Classification Loss: 1.9476\n",
      "2022-05-05 08:22:46,321 - root - INFO - Epoch: 3, Step: 2700, Average Loss: 2.9606, Average Regression Loss 1.0434, Average Classification Loss: 1.9173\n",
      "2022-05-05 08:23:01,397 - root - INFO - Epoch: 3, Step: 2800, Average Loss: 2.9672, Average Regression Loss 1.0199, Average Classification Loss: 1.9473\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "2022-05-05 08:23:25,587 - root - INFO - Epoch: 4, Step: 100, Average Loss: 2.8665, Average Regression Loss 0.9670, Average Classification Loss: 1.8995\n",
      "2022-05-05 08:23:40,783 - root - INFO - Epoch: 4, Step: 200, Average Loss: 2.8249, Average Regression Loss 0.9714, Average Classification Loss: 1.8536\n",
      "2022-05-05 08:23:56,169 - root - INFO - Epoch: 4, Step: 300, Average Loss: 3.0001, Average Regression Loss 1.0549, Average Classification Loss: 1.9452\n",
      "2022-05-05 08:24:11,174 - root - INFO - Epoch: 4, Step: 400, Average Loss: 2.8477, Average Regression Loss 0.9518, Average Classification Loss: 1.8959\n",
      "2022-05-05 08:24:27,068 - root - INFO - Epoch: 4, Step: 500, Average Loss: 3.0308, Average Regression Loss 1.0356, Average Classification Loss: 1.9952\n",
      "2022-05-05 08:24:42,966 - root - INFO - Epoch: 4, Step: 600, Average Loss: 2.8590, Average Regression Loss 0.9572, Average Classification Loss: 1.9018\n",
      "2022-05-05 08:24:58,559 - root - INFO - Epoch: 4, Step: 700, Average Loss: 2.8458, Average Regression Loss 0.9973, Average Classification Loss: 1.8485\n",
      "2022-05-05 08:25:14,010 - root - INFO - Epoch: 4, Step: 800, Average Loss: 2.7248, Average Regression Loss 0.8902, Average Classification Loss: 1.8346\n",
      "2022-05-05 08:25:29,325 - root - INFO - Epoch: 4, Step: 900, Average Loss: 2.8050, Average Regression Loss 0.9464, Average Classification Loss: 1.8585\n",
      "2022-05-05 08:25:45,614 - root - INFO - Epoch: 4, Step: 1000, Average Loss: 2.8234, Average Regression Loss 0.9558, Average Classification Loss: 1.8676\n",
      "2022-05-05 08:26:00,415 - root - INFO - Epoch: 4, Step: 1100, Average Loss: 2.8388, Average Regression Loss 0.9170, Average Classification Loss: 1.9218\n",
      "2022-05-05 08:26:16,364 - root - INFO - Epoch: 4, Step: 1200, Average Loss: 2.7088, Average Regression Loss 0.9377, Average Classification Loss: 1.7711\n",
      "2022-05-05 08:26:31,770 - root - INFO - Epoch: 4, Step: 1300, Average Loss: 2.8732, Average Regression Loss 1.0068, Average Classification Loss: 1.8664\n",
      "2022-05-05 08:26:46,408 - root - INFO - Epoch: 4, Step: 1400, Average Loss: 2.6804, Average Regression Loss 0.8727, Average Classification Loss: 1.8077\n",
      "2022-05-05 08:27:02,331 - root - INFO - Epoch: 4, Step: 1500, Average Loss: 2.8715, Average Regression Loss 1.0176, Average Classification Loss: 1.8538\n",
      "2022-05-05 08:27:18,103 - root - INFO - Epoch: 4, Step: 1600, Average Loss: 2.9775, Average Regression Loss 0.9561, Average Classification Loss: 2.0214\n",
      "2022-05-05 08:27:32,476 - root - INFO - Epoch: 4, Step: 1700, Average Loss: 2.8048, Average Regression Loss 0.9385, Average Classification Loss: 1.8662\n",
      "2022-05-05 08:27:47,564 - root - INFO - Epoch: 4, Step: 1800, Average Loss: 2.9386, Average Regression Loss 0.9819, Average Classification Loss: 1.9567\n",
      "tcmalloc: large alloc 2053177344 bytes == 0xec818000 @  0x7f0017d8c001 0x7eff5baa51af 0x7eff5bafbc23 0x7eff5bafca87 0x7eff5bb9e823 0x5936cc 0x548c51 0x5127f1 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x4bcb19 0x532e76 0x594b72 0x548cc1 0x51566f 0x4bc98a 0x532e76 0x594b72 0x548cc1 0x51566f 0x593dd7 0x511e2c 0x4bc98a 0x533274 0x4d3969 0x512147 0x4bc98a 0x533274\n",
      "2022-05-05 08:28:06,280 - root - INFO - Epoch: 4, Step: 1900, Average Loss: 2.9990, Average Regression Loss 1.0424, Average Classification Loss: 1.9566\n",
      "2022-05-05 08:28:24,599 - root - INFO - Epoch: 4, Step: 2000, Average Loss: 2.8609, Average Regression Loss 0.9974, Average Classification Loss: 1.8635\n",
      "2022-05-05 08:28:41,787 - root - INFO - Epoch: 4, Step: 2100, Average Loss: 2.8957, Average Regression Loss 0.9993, Average Classification Loss: 1.8964\n",
      "2022-05-05 08:28:58,343 - root - INFO - Epoch: 4, Step: 2200, Average Loss: 2.9688, Average Regression Loss 1.0464, Average Classification Loss: 1.9223\n",
      "2022-05-05 08:29:15,688 - root - INFO - Epoch: 4, Step: 2300, Average Loss: 2.7784, Average Regression Loss 0.9089, Average Classification Loss: 1.8696\n",
      "2022-05-05 08:29:32,367 - root - INFO - Epoch: 4, Step: 2400, Average Loss: 2.9968, Average Regression Loss 1.0440, Average Classification Loss: 1.9527\n",
      "2022-05-05 08:29:48,087 - root - INFO - Epoch: 4, Step: 2500, Average Loss: 2.9811, Average Regression Loss 1.0102, Average Classification Loss: 1.9709\n",
      "2022-05-05 08:30:04,842 - root - INFO - Epoch: 4, Step: 2600, Average Loss: 2.8650, Average Regression Loss 0.9551, Average Classification Loss: 1.9099\n",
      "2022-05-05 08:30:21,975 - root - INFO - Epoch: 4, Step: 2700, Average Loss: 2.8782, Average Regression Loss 0.9405, Average Classification Loss: 1.9377\n",
      "2022-05-05 08:30:37,177 - root - INFO - Epoch: 4, Step: 2800, Average Loss: 2.8735, Average Regression Loss 0.9630, Average Classification Loss: 1.9104\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "2022-05-05 08:31:01,525 - root - INFO - Epoch: 5, Step: 100, Average Loss: 2.9743, Average Regression Loss 1.0013, Average Classification Loss: 1.9730\n",
      "2022-05-05 08:31:16,876 - root - INFO - Epoch: 5, Step: 200, Average Loss: 2.8449, Average Regression Loss 0.9435, Average Classification Loss: 1.9014\n",
      "2022-05-05 08:31:32,558 - root - INFO - Epoch: 5, Step: 300, Average Loss: 2.8278, Average Regression Loss 0.9754, Average Classification Loss: 1.8524\n",
      "2022-05-05 08:31:47,880 - root - INFO - Epoch: 5, Step: 400, Average Loss: 2.8744, Average Regression Loss 0.9795, Average Classification Loss: 1.8949\n",
      "2022-05-05 08:32:03,618 - root - INFO - Epoch: 5, Step: 500, Average Loss: 2.8907, Average Regression Loss 0.9669, Average Classification Loss: 1.9238\n",
      "2022-05-05 08:32:18,667 - root - INFO - Epoch: 5, Step: 600, Average Loss: 2.8950, Average Regression Loss 0.9635, Average Classification Loss: 1.9315\n",
      "2022-05-05 08:32:34,218 - root - INFO - Epoch: 5, Step: 700, Average Loss: 2.8580, Average Regression Loss 0.9703, Average Classification Loss: 1.8877\n",
      "2022-05-05 08:32:50,192 - root - INFO - Epoch: 5, Step: 800, Average Loss: 2.7551, Average Regression Loss 0.9209, Average Classification Loss: 1.8342\n",
      "2022-05-05 08:33:06,794 - root - INFO - Epoch: 5, Step: 900, Average Loss: 2.9586, Average Regression Loss 1.0268, Average Classification Loss: 1.9318\n",
      "2022-05-05 08:33:22,621 - root - INFO - Epoch: 5, Step: 1000, Average Loss: 2.8138, Average Regression Loss 0.9815, Average Classification Loss: 1.8323\n",
      "2022-05-05 08:33:38,083 - root - INFO - Epoch: 5, Step: 1100, Average Loss: 2.9372, Average Regression Loss 0.9900, Average Classification Loss: 1.9472\n",
      "2022-05-05 08:33:52,745 - root - INFO - Epoch: 5, Step: 1200, Average Loss: 2.7628, Average Regression Loss 0.9350, Average Classification Loss: 1.8278\n",
      "2022-05-05 08:34:08,995 - root - INFO - Epoch: 5, Step: 1300, Average Loss: 2.7288, Average Regression Loss 0.9094, Average Classification Loss: 1.8193\n",
      "2022-05-05 08:34:24,624 - root - INFO - Epoch: 5, Step: 1400, Average Loss: 2.8064, Average Regression Loss 0.9520, Average Classification Loss: 1.8544\n",
      "2022-05-05 08:34:40,181 - root - INFO - Epoch: 5, Step: 1500, Average Loss: 2.8313, Average Regression Loss 0.9381, Average Classification Loss: 1.8932\n",
      "2022-05-05 08:34:54,859 - root - INFO - Epoch: 5, Step: 1600, Average Loss: 2.8137, Average Regression Loss 1.0079, Average Classification Loss: 1.8058\n",
      "2022-05-05 08:35:10,796 - root - INFO - Epoch: 5, Step: 1700, Average Loss: 2.8068, Average Regression Loss 0.9365, Average Classification Loss: 1.8703\n",
      "2022-05-05 08:35:27,220 - root - INFO - Epoch: 5, Step: 1800, Average Loss: 2.8416, Average Regression Loss 0.9608, Average Classification Loss: 1.8808\n",
      "2022-05-05 08:35:42,924 - root - INFO - Epoch: 5, Step: 1900, Average Loss: 3.0358, Average Regression Loss 1.0668, Average Classification Loss: 1.9691\n",
      "2022-05-05 08:35:59,037 - root - INFO - Epoch: 5, Step: 2000, Average Loss: 2.9433, Average Regression Loss 0.9965, Average Classification Loss: 1.9468\n",
      "2022-05-05 08:36:14,471 - root - INFO - Epoch: 5, Step: 2100, Average Loss: 2.8212, Average Regression Loss 0.9584, Average Classification Loss: 1.8627\n",
      "2022-05-05 08:36:30,857 - root - INFO - Epoch: 5, Step: 2200, Average Loss: 2.9203, Average Regression Loss 0.9726, Average Classification Loss: 1.9478\n",
      "2022-05-05 08:36:46,170 - root - INFO - Epoch: 5, Step: 2300, Average Loss: 2.9068, Average Regression Loss 0.9951, Average Classification Loss: 1.9117\n",
      "tcmalloc: large alloc 2156027904 bytes == 0xe9260000 @  0x7f0017d8c001 0x7eff5baa51af 0x7eff5bafbc23 0x7eff5bafca87 0x7eff5bb9e823 0x5936cc 0x548c51 0x5127f1 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x4bcb19 0x532e76 0x594b72 0x548cc1 0x51566f 0x4bc98a 0x532e76 0x594b72 0x548cc1 0x51566f 0x593dd7 0x511e2c 0x4bc98a 0x533274 0x4d3969 0x512147 0x4bc98a 0x533274\n",
      "tcmalloc: large alloc 2022440960 bytes == 0x16c1b8000 @  0x7f0017d8a1e7 0x7eff5baa50ce 0x7eff5baff726 0x7eff5baf2475 0x7eff4792ec30 0x7eff47bbd194 0x593784 0x548c51 0x51566f 0x549576 0x4bcb19 0x532e76 0x594b72 0x515600 0x549576 0x4bcb19 0x532e76 0x594b72 0x548cc1 0x51566f 0x4bc98a 0x532e76 0x594b72 0x548cc1 0x51566f 0x593dd7 0x511e2c 0x4bc98a 0x533274 0x4d3969 0x512147\n",
      "2022-05-05 08:37:05,838 - root - INFO - Epoch: 5, Step: 2400, Average Loss: 2.8262, Average Regression Loss 0.9357, Average Classification Loss: 1.8905\n",
      "2022-05-05 08:37:22,360 - root - INFO - Epoch: 5, Step: 2500, Average Loss: 2.7984, Average Regression Loss 0.9436, Average Classification Loss: 1.8548\n",
      "2022-05-05 08:37:37,974 - root - INFO - Epoch: 5, Step: 2600, Average Loss: 2.8796, Average Regression Loss 0.9681, Average Classification Loss: 1.9115\n",
      "2022-05-05 08:37:54,334 - root - INFO - Epoch: 5, Step: 2700, Average Loss: 2.7628, Average Regression Loss 0.9186, Average Classification Loss: 1.8442\n",
      "2022-05-05 08:38:09,844 - root - INFO - Epoch: 5, Step: 2800, Average Loss: 2.9004, Average Regression Loss 0.9591, Average Classification Loss: 1.9412\n",
      "2022-05-05 08:38:39,244 - root - INFO - Epoch: 5, Validation Loss: 2.0902, Validation Regression Loss 0.7064, Validation Classification Loss: 1.3837\n",
      "2022-05-05 08:38:39,308 - root - INFO - Saved model models/mb1-ssd-Epoch-5-Loss-2.0901970903007245.pth\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "2022-05-05 08:38:55,591 - root - INFO - Epoch: 6, Step: 100, Average Loss: 2.8894, Average Regression Loss 0.9745, Average Classification Loss: 1.9149\n",
      "2022-05-05 08:39:11,411 - root - INFO - Epoch: 6, Step: 200, Average Loss: 2.7815, Average Regression Loss 0.9238, Average Classification Loss: 1.8577\n",
      "2022-05-05 08:39:26,173 - root - INFO - Epoch: 6, Step: 300, Average Loss: 2.7151, Average Regression Loss 0.8931, Average Classification Loss: 1.8219\n",
      "2022-05-05 08:39:41,063 - root - INFO - Epoch: 6, Step: 400, Average Loss: 2.9421, Average Regression Loss 1.0544, Average Classification Loss: 1.8877\n",
      "2022-05-05 08:39:55,714 - root - INFO - Epoch: 6, Step: 500, Average Loss: 2.8047, Average Regression Loss 0.9503, Average Classification Loss: 1.8545\n",
      "2022-05-05 08:40:10,546 - root - INFO - Epoch: 6, Step: 600, Average Loss: 2.6550, Average Regression Loss 0.8440, Average Classification Loss: 1.8110\n",
      "2022-05-05 08:40:26,549 - root - INFO - Epoch: 6, Step: 700, Average Loss: 2.9707, Average Regression Loss 1.0592, Average Classification Loss: 1.9115\n",
      "2022-05-05 08:40:42,704 - root - INFO - Epoch: 6, Step: 800, Average Loss: 2.8214, Average Regression Loss 0.9292, Average Classification Loss: 1.8922\n",
      "2022-05-05 08:40:57,666 - root - INFO - Epoch: 6, Step: 900, Average Loss: 2.9508, Average Regression Loss 1.0389, Average Classification Loss: 1.9119\n",
      "2022-05-05 08:41:13,757 - root - INFO - Epoch: 6, Step: 1000, Average Loss: 2.7780, Average Regression Loss 0.9490, Average Classification Loss: 1.8290\n",
      "2022-05-05 08:41:29,430 - root - INFO - Epoch: 6, Step: 1100, Average Loss: 2.8952, Average Regression Loss 0.9899, Average Classification Loss: 1.9053\n",
      "2022-05-05 08:41:44,103 - root - INFO - Epoch: 6, Step: 1200, Average Loss: 2.8642, Average Regression Loss 0.9923, Average Classification Loss: 1.8718\n",
      "2022-05-05 08:42:00,600 - root - INFO - Epoch: 6, Step: 1300, Average Loss: 2.9191, Average Regression Loss 1.0057, Average Classification Loss: 1.9133\n",
      "2022-05-05 08:42:16,357 - root - INFO - Epoch: 6, Step: 1400, Average Loss: 2.8019, Average Regression Loss 0.9822, Average Classification Loss: 1.8197\n",
      "2022-05-05 08:42:31,623 - root - INFO - Epoch: 6, Step: 1500, Average Loss: 2.8101, Average Regression Loss 0.9406, Average Classification Loss: 1.8695\n",
      "2022-05-05 08:42:47,176 - root - INFO - Epoch: 6, Step: 1600, Average Loss: 2.8738, Average Regression Loss 0.9489, Average Classification Loss: 1.9249\n",
      "2022-05-05 08:43:02,232 - root - INFO - Epoch: 6, Step: 1700, Average Loss: 2.7956, Average Regression Loss 0.9215, Average Classification Loss: 1.8742\n",
      "2022-05-05 08:43:17,313 - root - INFO - Epoch: 6, Step: 1800, Average Loss: 2.7292, Average Regression Loss 0.9148, Average Classification Loss: 1.8144\n",
      "2022-05-05 08:43:31,956 - root - INFO - Epoch: 6, Step: 1900, Average Loss: 2.7673, Average Regression Loss 0.9019, Average Classification Loss: 1.8655\n",
      "2022-05-05 08:43:47,203 - root - INFO - Epoch: 6, Step: 2000, Average Loss: 2.8092, Average Regression Loss 0.9756, Average Classification Loss: 1.8336\n",
      "2022-05-05 08:44:03,842 - root - INFO - Epoch: 6, Step: 2100, Average Loss: 2.9467, Average Regression Loss 1.0393, Average Classification Loss: 1.9075\n",
      "2022-05-05 08:44:19,653 - root - INFO - Epoch: 6, Step: 2200, Average Loss: 2.8049, Average Regression Loss 0.9875, Average Classification Loss: 1.8174\n",
      "2022-05-05 08:44:34,536 - root - INFO - Epoch: 6, Step: 2300, Average Loss: 2.7911, Average Regression Loss 0.9331, Average Classification Loss: 1.8580\n",
      "2022-05-05 08:44:50,319 - root - INFO - Epoch: 6, Step: 2400, Average Loss: 2.5815, Average Regression Loss 0.8622, Average Classification Loss: 1.7193\n",
      "2022-05-05 08:45:05,327 - root - INFO - Epoch: 6, Step: 2500, Average Loss: 2.8994, Average Regression Loss 0.9730, Average Classification Loss: 1.9264\n",
      "2022-05-05 08:45:21,428 - root - INFO - Epoch: 6, Step: 2600, Average Loss: 2.9351, Average Regression Loss 0.9982, Average Classification Loss: 1.9369\n",
      "2022-05-05 08:45:36,857 - root - INFO - Epoch: 6, Step: 2700, Average Loss: 2.9646, Average Regression Loss 1.0182, Average Classification Loss: 1.9464\n",
      "2022-05-05 08:45:51,947 - root - INFO - Epoch: 6, Step: 2800, Average Loss: 2.8014, Average Regression Loss 0.9307, Average Classification Loss: 1.8707\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "2022-05-05 08:46:15,783 - root - INFO - Epoch: 7, Step: 100, Average Loss: 2.9194, Average Regression Loss 0.9946, Average Classification Loss: 1.9248\n",
      "2022-05-05 08:46:30,896 - root - INFO - Epoch: 7, Step: 200, Average Loss: 2.7494, Average Regression Loss 0.9221, Average Classification Loss: 1.8272\n",
      "2022-05-05 08:46:45,757 - root - INFO - Epoch: 7, Step: 300, Average Loss: 2.9127, Average Regression Loss 1.0028, Average Classification Loss: 1.9099\n",
      "2022-05-05 08:47:00,817 - root - INFO - Epoch: 7, Step: 400, Average Loss: 2.7792, Average Regression Loss 0.9312, Average Classification Loss: 1.8481\n",
      "2022-05-05 08:47:16,073 - root - INFO - Epoch: 7, Step: 500, Average Loss: 2.9074, Average Regression Loss 1.0134, Average Classification Loss: 1.8940\n",
      "2022-05-05 08:47:31,380 - root - INFO - Epoch: 7, Step: 600, Average Loss: 2.7515, Average Regression Loss 0.9074, Average Classification Loss: 1.8441\n",
      "2022-05-05 08:47:46,060 - root - INFO - Epoch: 7, Step: 700, Average Loss: 2.8340, Average Regression Loss 0.9756, Average Classification Loss: 1.8584\n",
      "2022-05-05 08:48:01,903 - root - INFO - Epoch: 7, Step: 800, Average Loss: 2.8093, Average Regression Loss 0.9190, Average Classification Loss: 1.8903\n",
      "2022-05-05 08:48:17,372 - root - INFO - Epoch: 7, Step: 900, Average Loss: 2.9052, Average Regression Loss 0.9866, Average Classification Loss: 1.9187\n",
      "2022-05-05 08:48:34,142 - root - INFO - Epoch: 7, Step: 1000, Average Loss: 2.6648, Average Regression Loss 0.8621, Average Classification Loss: 1.8027\n",
      "2022-05-05 08:48:48,895 - root - INFO - Epoch: 7, Step: 1100, Average Loss: 2.6399, Average Regression Loss 0.8822, Average Classification Loss: 1.7577\n",
      "2022-05-05 08:49:04,185 - root - INFO - Epoch: 7, Step: 1200, Average Loss: 2.6959, Average Regression Loss 0.8752, Average Classification Loss: 1.8207\n",
      "2022-05-05 08:49:19,638 - root - INFO - Epoch: 7, Step: 1300, Average Loss: 2.8505, Average Regression Loss 0.9469, Average Classification Loss: 1.9036\n",
      "2022-05-05 08:49:35,190 - root - INFO - Epoch: 7, Step: 1400, Average Loss: 2.7969, Average Regression Loss 0.9528, Average Classification Loss: 1.8442\n",
      "2022-05-05 08:49:51,986 - root - INFO - Epoch: 7, Step: 1500, Average Loss: 2.8195, Average Regression Loss 0.9757, Average Classification Loss: 1.8439\n",
      "2022-05-05 08:50:07,319 - root - INFO - Epoch: 7, Step: 1600, Average Loss: 2.6942, Average Regression Loss 0.8855, Average Classification Loss: 1.8087\n",
      "2022-05-05 08:50:23,130 - root - INFO - Epoch: 7, Step: 1700, Average Loss: 2.8515, Average Regression Loss 0.9826, Average Classification Loss: 1.8688\n",
      "2022-05-05 08:50:39,236 - root - INFO - Epoch: 7, Step: 1800, Average Loss: 2.9068, Average Regression Loss 0.9760, Average Classification Loss: 1.9308\n",
      "2022-05-05 08:50:54,225 - root - INFO - Epoch: 7, Step: 1900, Average Loss: 2.8898, Average Regression Loss 0.9795, Average Classification Loss: 1.9103\n",
      "2022-05-05 08:51:11,041 - root - INFO - Epoch: 7, Step: 2000, Average Loss: 2.7728, Average Regression Loss 0.9104, Average Classification Loss: 1.8623\n",
      "2022-05-05 08:51:27,609 - root - INFO - Epoch: 7, Step: 2100, Average Loss: 2.8110, Average Regression Loss 0.9450, Average Classification Loss: 1.8660\n",
      "2022-05-05 08:51:44,563 - root - INFO - Epoch: 7, Step: 2200, Average Loss: 2.8041, Average Regression Loss 0.9416, Average Classification Loss: 1.8625\n",
      "2022-05-05 08:52:01,473 - root - INFO - Epoch: 7, Step: 2300, Average Loss: 2.7899, Average Regression Loss 0.9071, Average Classification Loss: 1.8828\n",
      "2022-05-05 08:52:17,730 - root - INFO - Epoch: 7, Step: 2400, Average Loss: 2.7577, Average Regression Loss 0.9182, Average Classification Loss: 1.8395\n",
      "2022-05-05 08:52:34,390 - root - INFO - Epoch: 7, Step: 2500, Average Loss: 2.7720, Average Regression Loss 0.9477, Average Classification Loss: 1.8243\n",
      "2022-05-05 08:52:50,871 - root - INFO - Epoch: 7, Step: 2600, Average Loss: 2.7857, Average Regression Loss 1.0034, Average Classification Loss: 1.7823\n",
      "2022-05-05 08:53:07,995 - root - INFO - Epoch: 7, Step: 2700, Average Loss: 2.9553, Average Regression Loss 1.0255, Average Classification Loss: 1.9298\n",
      "2022-05-05 08:53:24,889 - root - INFO - Epoch: 7, Step: 2800, Average Loss: 2.6483, Average Regression Loss 0.8723, Average Classification Loss: 1.7760\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "/content/pytorch-ssd/vision/transforms/transforms.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mode = random.choice(self.sample_options)\n",
      "2022-05-05 08:53:48,250 - root - INFO - Epoch: 8, Step: 100, Average Loss: 2.7876, Average Regression Loss 0.9404, Average Classification Loss: 1.8472\n",
      "2022-05-05 08:54:03,854 - root - INFO - Epoch: 8, Step: 200, Average Loss: 2.8043, Average Regression Loss 0.9449, Average Classification Loss: 1.8594\n",
      "2022-05-05 08:54:19,400 - root - INFO - Epoch: 8, Step: 300, Average Loss: 2.8155, Average Regression Loss 0.9419, Average Classification Loss: 1.8735\n",
      "2022-05-05 08:54:35,496 - root - INFO - Epoch: 8, Step: 400, Average Loss: 2.7885, Average Regression Loss 0.9583, Average Classification Loss: 1.8302\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eff4e3d8ef0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1322, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/popen_fork.py\", line 45, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Traceback (most recent call last):\n",
      "  File \"pytorch-ssd/train_ssd.py\", line 326, in <module>\n",
      "    device=DEVICE, debug_steps=args.debug_steps, epoch=epoch)\n",
      "  File \"pytorch-ssd/train_ssd.py\", line 118, in train\n",
      "    images = images.to(device)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 pytorch-ssd/train_ssd.py --dataset_type open_images --datasets open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001  --batch_size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sutzWx87TvNa"
   },
   "outputs": [],
   "source": [
    "!cp ./models gdrive/MyDrive/DLS-Homework5/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMb82zV3TcrT"
   },
   "source": [
    "### Testing without finetunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pj6Rmt40QFZq",
    "outputId": "797081a3-0f08-4824-d61b-b448abce0a1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"pytorch-ssd/eval_ssd.py\", line 127, in <module>\n",
      "    class_names = [name.strip() for name in open(args.label_file).readlines()]\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'models/open-images-model-labels.txt'\n"
     ]
    }
   ],
   "source": [
    "!python pytorch-ssd/eval_ssd.py --net mb1-ssd --dataset open_images --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/open-images-model-labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5FYBGJpF8u_"
   },
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAcDdfOh44yi"
   },
   "outputs": [],
   "source": [
    "!pip install onnx\n",
    "\n",
    "# some_file.py\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, 'pytorch-ssd')\n",
    "\n",
    "from vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd\n",
    "import torch.onnx\n",
    "import onnx\n",
    "\n",
    "model_path = \"models/mobilenet-v1-ssd-mp-0_675.pth\"\n",
    "label_path = \"models/voc-model-labels.txt\"\n",
    "\n",
    "class_names = [name.strip() for name in open(label_path).readlines()]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "net = create_mobilenetv1_ssd(len(class_names), is_test=True)\n",
    "net.load(model_path)\n",
    "net = net.to(\"cuda\")\n",
    "net.eval()\n",
    "net_type = \"mb1-ssd\"\n",
    "model_path = f\"models/{net_type}.onnx\"\n",
    "# init_net_path = f\"models/{net_type}_init_net.pb\"\n",
    "# init_net_txt_path = f\"models/{net_type}_init_net.pbtxt\"\n",
    "# predict_net_path = f\"models/{net_type}_predict_net.pb\"\n",
    "# predict_net_txt_path = f\"models/{net_type}_predict_net.pbtxt\"\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 300, 300, device=\"cuda\")\n",
    "torch.onnx.export(net, dummy_input, model_path, verbose=False, output_names=['scores', 'boxes'])\n",
    "\n",
    "model = onnx.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDJ5TjekS2xG"
   },
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndRVX-BLGby-"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/onnx/onnx.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ht4ewGz3RbhK"
   },
   "outputs": [],
   "source": [
    "!python onnx/onnx/tools/net_drawer.py --input models/mb1-ssd.onnx --output mb1-ssd.dot --embed_docstring\n",
    "!dot -Tsvg mb1-ssd.dot -o mb1-ssd.svg\n",
    "!cp mb1-ssd.dot gdrive/MyDrive/DLS-Homework5/mb1-ssd.dot \n",
    "!cp mb1-ssd.svg gdrive/MyDrive/DLS-Homework5/mb1-ssd.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Q7UwtXZMTT"
   },
   "source": [
    "The stack trace for nodes of different types are: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NANU6DRgZPi-"
   },
   "source": [
    "TODO: add stack trace from svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYGjF3zrZXLy"
   },
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVcTQnBDWD4o"
   },
   "outputs": [],
   "source": [
    "!docker pull mcr.microsoft.com/onnxruntime/server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Km60ka_Tbqx3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "problem1.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
